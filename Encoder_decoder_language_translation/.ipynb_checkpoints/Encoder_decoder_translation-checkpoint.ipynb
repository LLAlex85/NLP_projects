{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language translation with encoder-decoder model\n",
    "\n",
    "This notebook is based on the programming assignment \"Machine translation\" of the Udacity Natural Language Processing Nanodegree. \n",
    "\n",
    "In this notebook I am implementing a sequence-to-sequence model to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French). The network architecture is a GRU encoder-decoder model without attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2916401904031470286\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 357433344\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12616659151367246620\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of English-French translation dataset\n",
    "\n",
    "The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`. The sentences have been preprocessed already. The puncuations have been delimited using spaces and all the text has been converted to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "We convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids.\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "\n",
    "### Tokenize\n",
    "We turn each sentence into a sequence of words ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "\n",
    "    x_tk = Tokenizer()\n",
    "    x_tk.fit_on_texts(x)\n",
    "    \n",
    "    return x_tk.texts_to_sequences(x), x_tk\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "All sequences in each batch need to have the same length. Since sentences are dynamic in length, we add padding to the end of the sequences to make them the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "\n",
    "    # length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    if length is None:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Ids Back to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "\n",
    "    # Turn logits from the neural network into text using the tokenizer\n",
    "\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    # String that represents the text of the logits\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "The encoder-decoder model architecture that we implement is shown in the next figure.\n",
    "<img src=\"images/seq2seq.png\" style=\"width:180;height:180px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 32)       6400        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 32)     11040       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256)          123648      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     [(None, None, 256),  221952      embedding_2[0][0]                \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 345)    88665       gru_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 451,705\n",
      "Trainable params: 451,705\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/100\n",
      "110288/110288 [==============================] - 19s 171us/step - loss: 3.2200 - acc: 0.4167 - val_loss: 2.4906 - val_acc: 0.4785\n",
      "Epoch 2/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 2.1434 - acc: 0.5234 - val_loss: 1.6152 - val_acc: 0.5960\n",
      "Epoch 3/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 1.2898 - acc: 0.6504 - val_loss: 1.0650 - val_acc: 0.6889\n",
      "Epoch 4/100\n",
      "110288/110288 [==============================] - 18s 166us/step - loss: 0.9169 - acc: 0.7226 - val_loss: 0.8077 - val_acc: 0.7488\n",
      "Epoch 5/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.7461 - acc: 0.7616 - val_loss: 0.6986 - val_acc: 0.7733\n",
      "Epoch 6/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.6623 - acc: 0.7810 - val_loss: 0.6304 - val_acc: 0.7879\n",
      "Epoch 7/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.6062 - acc: 0.7925 - val_loss: 0.5878 - val_acc: 0.7958\n",
      "Epoch 8/100\n",
      "110288/110288 [==============================] - 18s 163us/step - loss: 0.5521 - acc: 0.8069 - val_loss: 0.5215 - val_acc: 0.8155\n",
      "Epoch 9/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.4991 - acc: 0.8226 - val_loss: 0.4762 - val_acc: 0.8313\n",
      "Epoch 10/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.4602 - acc: 0.8343 - val_loss: 0.4409 - val_acc: 0.8390\n",
      "Epoch 11/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.4180 - acc: 0.8499 - val_loss: 0.3971 - val_acc: 0.8603\n",
      "Epoch 12/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.3865 - acc: 0.8627 - val_loss: 0.3864 - val_acc: 0.8577\n",
      "Epoch 13/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.3661 - acc: 0.8695 - val_loss: 0.3494 - val_acc: 0.8772\n",
      "Epoch 14/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.3402 - acc: 0.8818 - val_loss: 0.3231 - val_acc: 0.8904\n",
      "Epoch 15/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.3157 - acc: 0.8927 - val_loss: 0.2995 - val_acc: 0.8996\n",
      "Epoch 16/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2909 - acc: 0.9009 - val_loss: 0.2812 - val_acc: 0.9048\n",
      "Epoch 17/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2750 - acc: 0.9060 - val_loss: 0.2687 - val_acc: 0.9084\n",
      "Epoch 18/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2961 - acc: 0.8988 - val_loss: 0.2762 - val_acc: 0.9037\n",
      "Epoch 19/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2568 - acc: 0.9123 - val_loss: 0.2502 - val_acc: 0.9140\n",
      "Epoch 20/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2451 - acc: 0.9158 - val_loss: 0.2424 - val_acc: 0.9165\n",
      "Epoch 21/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2373 - acc: 0.9183 - val_loss: 0.2396 - val_acc: 0.9154\n",
      "Epoch 22/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2306 - acc: 0.9203 - val_loss: 0.2274 - val_acc: 0.9224\n",
      "Epoch 23/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2475 - acc: 0.9147 - val_loss: 0.2289 - val_acc: 0.9211\n",
      "Epoch 24/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2192 - acc: 0.9245 - val_loss: 0.2156 - val_acc: 0.9260\n",
      "Epoch 25/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2102 - acc: 0.9277 - val_loss: 0.2070 - val_acc: 0.9291\n",
      "Epoch 26/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.2118 - acc: 0.9268 - val_loss: 0.2027 - val_acc: 0.9295\n",
      "Epoch 27/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.1918 - acc: 0.9341 - val_loss: 0.1908 - val_acc: 0.9331\n",
      "Epoch 28/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.1903 - acc: 0.9336 - val_loss: 0.1889 - val_acc: 0.9338\n",
      "Epoch 29/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.1714 - acc: 0.9400 - val_loss: 0.1706 - val_acc: 0.9384\n",
      "Epoch 30/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.1634 - acc: 0.9427 - val_loss: 0.1547 - val_acc: 0.9467\n",
      "Epoch 31/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.1529 - acc: 0.9468 - val_loss: 0.1531 - val_acc: 0.9456\n",
      "Epoch 32/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.1434 - acc: 0.9512 - val_loss: 0.1354 - val_acc: 0.9549\n",
      "Epoch 33/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.1352 - acc: 0.9542 - val_loss: 0.1266 - val_acc: 0.9588\n",
      "Epoch 34/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.1246 - acc: 0.9584 - val_loss: 0.1229 - val_acc: 0.9580\n",
      "Epoch 35/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.1174 - acc: 0.9610 - val_loss: 0.1546 - val_acc: 0.9529\n",
      "Epoch 36/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.1123 - acc: 0.9629 - val_loss: 0.1214 - val_acc: 0.9585\n",
      "Epoch 37/100\n",
      "110288/110288 [==============================] - 18s 164us/step - loss: 0.1072 - acc: 0.9645 - val_loss: 0.1098 - val_acc: 0.9620\n",
      "Epoch 38/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.1020 - acc: 0.9659 - val_loss: 0.1025 - val_acc: 0.9646\n",
      "Epoch 39/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0991 - acc: 0.9675 - val_loss: 0.0988 - val_acc: 0.9660\n",
      "Epoch 40/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0936 - acc: 0.9689 - val_loss: 0.1034 - val_acc: 0.9648\n",
      "Epoch 41/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0900 - acc: 0.9699 - val_loss: 0.0890 - val_acc: 0.9715\n",
      "Epoch 42/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0887 - acc: 0.9709 - val_loss: 0.0920 - val_acc: 0.9684\n",
      "Epoch 43/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0832 - acc: 0.9727 - val_loss: 0.0787 - val_acc: 0.9750\n",
      "Epoch 44/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0787 - acc: 0.9751 - val_loss: 0.0677 - val_acc: 0.9817\n",
      "Epoch 45/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0742 - acc: 0.9769 - val_loss: 0.0740 - val_acc: 0.9764\n",
      "Epoch 46/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0751 - acc: 0.9773 - val_loss: 0.0724 - val_acc: 0.9776\n",
      "Epoch 47/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0586 - acc: 0.9837 - val_loss: 0.1159 - val_acc: 0.9638\n",
      "Epoch 48/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0647 - acc: 0.9801 - val_loss: 0.0537 - val_acc: 0.9856\n",
      "Epoch 49/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0503 - acc: 0.9863 - val_loss: 0.0511 - val_acc: 0.9861\n",
      "Epoch 50/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0847 - acc: 0.9757 - val_loss: 0.0532 - val_acc: 0.9857\n",
      "Epoch 51/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0472 - acc: 0.9873 - val_loss: 0.0476 - val_acc: 0.9872\n",
      "Epoch 52/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0445 - acc: 0.9878 - val_loss: 0.0485 - val_acc: 0.9864\n",
      "Epoch 53/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0425 - acc: 0.9883 - val_loss: 0.0640 - val_acc: 0.9779\n",
      "Epoch 54/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0965 - acc: 0.9750 - val_loss: 0.0457 - val_acc: 0.9877\n",
      "Epoch 55/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0409 - acc: 0.9888 - val_loss: 0.0424 - val_acc: 0.9884\n",
      "Epoch 56/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0391 - acc: 0.9891 - val_loss: 0.0413 - val_acc: 0.9884\n",
      "Epoch 57/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0677 - acc: 0.9813 - val_loss: 0.0885 - val_acc: 0.9691\n",
      "Epoch 58/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0471 - acc: 0.9858 - val_loss: 0.0403 - val_acc: 0.9888\n",
      "Epoch 59/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0366 - acc: 0.9897 - val_loss: 0.0386 - val_acc: 0.9892\n",
      "Epoch 60/100\n",
      "110288/110288 [==============================] - 18s 162us/step - loss: 0.0354 - acc: 0.9899 - val_loss: 0.0376 - val_acc: 0.9894\n",
      "Epoch 61/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0637 - acc: 0.9827 - val_loss: 0.0377 - val_acc: 0.9894\n",
      "Epoch 62/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0342 - acc: 0.9903 - val_loss: 0.0364 - val_acc: 0.9897\n",
      "Epoch 63/100\n",
      "110288/110288 [==============================] - 18s 159us/step - loss: 0.0332 - acc: 0.9905 - val_loss: 0.0354 - val_acc: 0.9899\n",
      "Epoch 64/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0323 - acc: 0.9906 - val_loss: 0.0347 - val_acc: 0.9900\n",
      "Epoch 65/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0317 - acc: 0.9908 - val_loss: 0.0340 - val_acc: 0.9902\n",
      "Epoch 66/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0859 - acc: 0.9771 - val_loss: 0.0374 - val_acc: 0.9894\n",
      "Epoch 67/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0321 - acc: 0.9908 - val_loss: 0.0339 - val_acc: 0.9904\n",
      "Epoch 68/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0304 - acc: 0.9912 - val_loss: 0.0331 - val_acc: 0.9905\n",
      "Epoch 69/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0295 - acc: 0.9913 - val_loss: 0.0323 - val_acc: 0.9906\n",
      "Epoch 70/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0290 - acc: 0.9915 - val_loss: 0.0316 - val_acc: 0.9908\n",
      "Epoch 71/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0848 - acc: 0.9787 - val_loss: 0.0337 - val_acc: 0.9905\n",
      "Epoch 72/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0293 - acc: 0.9914 - val_loss: 0.0316 - val_acc: 0.9908\n",
      "Epoch 73/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0279 - acc: 0.9918 - val_loss: 0.0306 - val_acc: 0.9911\n",
      "Epoch 74/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0272 - acc: 0.9920 - val_loss: 0.0298 - val_acc: 0.9913\n",
      "Epoch 75/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0266 - acc: 0.9921 - val_loss: 0.0294 - val_acc: 0.9915\n",
      "Epoch 76/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0641 - acc: 0.9831 - val_loss: 0.0385 - val_acc: 0.9882\n",
      "Epoch 77/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0282 - acc: 0.9917 - val_loss: 0.0297 - val_acc: 0.9913\n",
      "Epoch 78/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0260 - acc: 0.9923 - val_loss: 0.0289 - val_acc: 0.9913\n",
      "Epoch 79/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0252 - acc: 0.9925 - val_loss: 0.0283 - val_acc: 0.9915\n",
      "Epoch 80/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0248 - acc: 0.9926 - val_loss: 0.0279 - val_acc: 0.9918\n",
      "Epoch 81/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0243 - acc: 0.9927 - val_loss: 0.0275 - val_acc: 0.9919\n",
      "Epoch 82/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0874 - acc: 0.9768 - val_loss: 0.0442 - val_acc: 0.9855\n",
      "Epoch 83/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0384 - acc: 0.9878 - val_loss: 0.0288 - val_acc: 0.9916\n",
      "Epoch 84/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0248 - acc: 0.9927 - val_loss: 0.0275 - val_acc: 0.9919\n",
      "Epoch 85/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0236 - acc: 0.9931 - val_loss: 0.0267 - val_acc: 0.9921\n",
      "Epoch 86/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0261 - val_acc: 0.9923\n",
      "Epoch 87/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0257 - val_acc: 0.9925\n",
      "Epoch 88/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0220 - acc: 0.9935 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 89/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0216 - acc: 0.9936 - val_loss: 0.0252 - val_acc: 0.9925\n",
      "Epoch 90/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0213 - acc: 0.9937 - val_loss: 0.0245 - val_acc: 0.9929\n",
      "Epoch 91/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0253 - val_acc: 0.9924\n",
      "Epoch 92/100\n",
      "110288/110288 [==============================] - 18s 160us/step - loss: 0.0757 - acc: 0.9822 - val_loss: 0.0260 - val_acc: 0.9925\n",
      "Epoch 93/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0216 - acc: 0.9937 - val_loss: 0.0245 - val_acc: 0.9928\n",
      "Epoch 94/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0206 - acc: 0.9940 - val_loss: 0.0243 - val_acc: 0.9929\n",
      "Epoch 95/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0200 - acc: 0.9942 - val_loss: 0.0236 - val_acc: 0.9932\n",
      "Epoch 96/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0195 - acc: 0.9943 - val_loss: 0.0230 - val_acc: 0.9935\n",
      "Epoch 97/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0193 - acc: 0.9944 - val_loss: 0.0230 - val_acc: 0.9933\n",
      "Epoch 98/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0368 - acc: 0.9915 - val_loss: 0.2423 - val_acc: 0.9442\n",
      "Epoch 99/100\n",
      "110288/110288 [==============================] - 18s 164us/step - loss: 0.0412 - acc: 0.9878 - val_loss: 0.0238 - val_acc: 0.9931\n",
      "Epoch 100/100\n",
      "110288/110288 [==============================] - 18s 161us/step - loss: 0.0192 - acc: 0.9946 - val_loss: 0.0226 - val_acc: 0.9936\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "embedding_size = 32\n",
    "gru_units = 128\n",
    "\n",
    "# encoder\n",
    "encoder_input_shape = preproc_english_sentences.shape\n",
    "\n",
    "encoder_input = Input(shape=(encoder_input_shape[1], ))\n",
    "encoder_embedding = Embedding(english_vocab_size+1, embedding_size, input_length=encoder_input_shape[1])\n",
    "encoder_outputs = encoder_embedding(encoder_input)\n",
    "\n",
    "encoder_bidrectional_GRU = Bidirectional(GRU(gru_units, return_state = False))\n",
    "encoder_outputs = encoder_bidrectional_GRU(encoder_outputs)\n",
    "encoder_state = encoder_outputs\n",
    "\n",
    "# decoder\n",
    "decoder_input = Input(shape=(None, ))\n",
    "decoder_embedding = Embedding(french_vocab_size+1, embedding_size)\n",
    "decoder_outputs = decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_GRU = GRU(2*gru_units, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _ = decoder_GRU(decoder_outputs, initial_state = encoder_state)\n",
    "\n",
    "decoder_dense = TimeDistributed(Dense(french_vocab_size+1, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs) \n",
    "\n",
    "encoder_decoder_model = Model(inputs = [encoder_input, decoder_input], outputs = decoder_outputs)\n",
    "encoder_decoder_model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "encoder_decoder_model.summary()\n",
    "\n",
    "\n",
    "preproc_french_sentences_decinp = np.zeros((preproc_french_sentences.shape))\n",
    "preproc_french_sentences_decinp[:,1:,:] = preproc_french_sentences[:,0:-1,:]\n",
    "\n",
    "num_epochs = 100\n",
    "encoder_decoder_model.fit([preproc_english_sentences, preproc_french_sentences_decinp[:,:,0]], preproc_french_sentences, \n",
    "                           batch_size=1024, epochs= num_epochs, validation_split=0.2)\n",
    "encoder_decoder_model.save_weights(\"encoder_decoder_model_\"+str(num_epochs)+\"_epochs_v2.h5\")\n",
    "\n",
    "print(logits_to_text(encoder_decoder_model.predict([preproc_english_sentences[:1], \n",
    "                                                   preproc_french_sentences_decinp[:1,:,0]])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, encoder_state)\n",
    "\n",
    "decoder_state_input = Input(shape=(2*gru_units,))\n",
    "decoder_outputs = decoder_embedding(decoder_input)\n",
    "decoder_outputs, decoder_state = decoder_GRU(decoder_outputs, initial_state = decoder_state_input)\n",
    "decoder_outputs = decoder_dense(decoder_outputs) \n",
    "decoder_model = Model([decoder_input, decoder_state_input], [decoder_outputs, decoder_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk, sentence):\n",
    "    \n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    \n",
    "    encoder_state_value = encoder_model.predict(sentence)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    decoded_sentence = ''\n",
    "    decoder_state_ = encoder_state_value\n",
    "\n",
    "    target_seq_len = 0\n",
    "    while (target_seq_len < 21):\n",
    "        decoder_output_, decoder_state_ = decoder_model.predict([target_seq, decoder_state_])\n",
    "        decoded_sentence += y_id_to_word[np.argmax(decoder_output_)]+' '\n",
    "    \n",
    "        target_seq = np.argmax(decoder_output_).reshape(1,1)\n",
    "        target_seq_len += 1\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril\n"
     ]
    }
   ],
   "source": [
    "sentence = 'he saw a old yellow truck'\n",
    "decoded_sentence = final_predictions(preproc_english_sentences, preproc_french_sentences, \n",
    "                                       english_tokenizer, french_tokenizer, sentence)\n",
    "\n",
    "print('Sample 1:')\n",
    "print(decoded_sentence)\n",
    "print('Il a vu un vieux camion jaune')\n",
    "\n",
    "sentence = 'new jersey is sometimes quiet during autumn and it is snowy in april'\n",
    "decoded_sentence = final_predictions(preproc_english_sentences, preproc_french_sentences, \n",
    "                                       english_tokenizer, french_tokenizer, sentence)\n",
    "\n",
    "print('Sample 2:')\n",
    "print(decoded_sentence)\n",
    "print('new jersey est parfois calme pendant l\\' automne et il est neigeux en avril')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
