{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger word detection using GRU\n",
    "\n",
    "This notebook is based on the programming assignment \"Trigger Word Detection\" of deeplearning.ai, course Sequence models, week Sequence models and Attention mechanism. The figures are also taken from that assignment.\n",
    "\n",
    "We will implement a network architecture using GRUs for trigger word detection. Trigger word detection is the technology that allows devices like Amazon Alexa, Google Home, Apple Siri to wake up upon hearing a certain word.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "- Application of Gated Rrecurrent Units (GRU) in TensorFlow\n",
    "- Understand GRU architecture difference between Keras and TesnorFlow\n",
    "- Apply Batch Normalization\n",
    "- Use Adam optimizer with decay rate\n",
    "- Convert hd5 file format to TensorFlow ckpt file\n",
    "- Load Keras pre-trained weights to TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 5511, 101)\n",
      "(26, 1375, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed training examples\n",
    "X = np.load(\"./XY_train/X.npy\")\n",
    "Y = np.load(\"./XY_train/Y.npy\")\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 5511, 101)\n",
      "(25, 1375, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed dev set examples\n",
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")\n",
    "\n",
    "print(X_dev.shape)\n",
    "print(Y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert kernel and recurrent_kernel from keras to gate_kernel and candidate_kernel in TensorFlow\n",
    "\n",
    "We split the kernel and recurrent_kernel from keras into the z (update gate), r (reset gate) and h (hidden layer). From the keras documentation self.kernel has size (inputs, units\\*3) and self.recurrent_kernel has size (units, units\\*3). \n",
    "\n",
    "From self.kernel we get 3 kernels with size (input, units) i.e. self.kernel_z=self.kernel(input,:units), self.kernel_r=self.kernel(input,units:units\\*2) and self.kernel_h=self.kernel(input,units\\*2:)\n",
    "\n",
    "From self.recurrent_kernel we get 3 kernels with size (units, units) i.e. self.recurrent_kernel_z=self.recurrent_kernel(units,:units), self.recurrent_kernel_r=self.recurrent_kernel(units,units:units\\*2) and self.recurrent_kernel_h=self.recurrent_kernel(units,units\\*2:)\n",
    "\n",
    "From the TensorFlow documentation gate_kernel has size (input+units, 2\\*units), gate_bias has size 2\\*units, candidate_kernel has size (input+units,units) and candidate_bias has size (units)\n",
    "\n",
    "From the TensorFlow documentation\n",
    "\n",
    "gate_inputs = math_ops.matmul(array_ops.concat([inputs, state], 1), self._gate_kernel)\n",
    "\n",
    "gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)\n",
    "\n",
    "value = math_ops.sigmoid(gate_inputs)\n",
    "\n",
    "r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n",
    "\n",
    "Since gate_inputs = math_ops.matmul(array_ops.concat([inputs, state], 1), self._gate_kernel) that means self._gate_kernel)  should be concatenation of kernel and recurrent_kernel with this order. \n",
    "\n",
    "Since r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1) that means that self._gate_kernel includes gates r and z with r first and z second (opposite from keras)\n",
    "\n",
    "Similarly\n",
    "\n",
    "candidate = math_ops.matmul(array_ops.concat([inputs, r_state], 1), self._candidate_kernel)\n",
    "candidate = nn_ops.bias_add(candidate, self._candidate_bias)\n",
    "\n",
    "where self._candidate_kernel corresponds to candidate hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert kernel and recurrent_kernel from keras to gate_kernel and candidate_kernel in TensorFlow\n",
    "\n",
    "def convert_kernel(kernel):\n",
    "    kernel_z, kernel_r, kernel_h = np.hsplit(kernel, 3)\n",
    "    return kernel_z, kernel_r, kernel_h\n",
    "\n",
    "def gate_and_candidate_kernel(kernel,recurrent_kernel):\n",
    "    kernel_z, kernel_r, kernel_h=convert_kernel(kernel[0])\n",
    "    recurrent_kernel_z, recurrent_kernel_r, recurrent_kernel_h=convert_kernel(recurrent_kernel[0])\n",
    "\n",
    "    r_concat=np.concatenate([kernel_r,recurrent_kernel_r],axis=0)\n",
    "    z_concat=np.concatenate([kernel_z,recurrent_kernel_z],axis=0)\n",
    "    gate_kernel=np.concatenate([r_concat,z_concat],axis=1)\n",
    "\n",
    "    candidate_kernel=np.concatenate([kernel_h,recurrent_kernel_h],axis=0)\n",
    "\n",
    "    return gate_kernel, candidate_kernel\n",
    "\n",
    "def convert_bias(bias):\n",
    "    bias = bias.reshape(3, -1) \n",
    "    dim2 = bias.shape[1]\n",
    "    bias = bias[[1, 0, 2], :].reshape(-1)\n",
    "    bias_1 = bias[:dim2*2]\n",
    "    bias_2 = bias[dim2*2:]\n",
    "    return bias_1, bias_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(Tx, n_freq, Ty):\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.float32,[None,Tx,n_freq],name='inputs_')\n",
    "    targets_ = tf.placeholder(tf.float32,[None,Ty,1], name='targets_')\n",
    "    training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs_, targets_, training, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_gru(gru_size, gru_layers, batch_size, keep_prob):\n",
    "    \n",
    "    # Your basic GRU cell\n",
    "    gru = tf.contrib.rnn.GRUCell(gru_size) \n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(gru, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple GRU layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop for _ in range(gru_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets):\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, decay_rate):\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decay_steps = 1.0\n",
    "    learning_rate = tf.train.inverse_time_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)\n",
    "    \n",
    "    return optimizer, learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network architecture\n",
    "The model will use 1-D convolutional layers, GRU layers, and dense layers.\n",
    "<img src=\"images/model_.png\" style=\"width:600px;height:600px;\">\n",
    "\n",
    "One key step of this model is the 1D convolutional step (near the bottom of Figure). It inputs the 5511 step spectrogram, and outputs a 1375 step output, which is then further processed by multiple layers to get the final $T_y = 1375$ step output. This layer plays a role similar to the 2D convolutions, of extracting low-level features and then possibly generating an output of a smaller dimension. \n",
    "\n",
    "Computationally, the 1-D conv layer also helps speed up the model because now the GRU  has to process only 1375 timesteps rather than 5511 timesteps. The two GRU layers read the sequence of inputs from left to right, then ultimately uses a dense+sigmoid layer to make a prediction for $y^{\\langle t \\rangle}$. Because $y$ is binary valued (0 or 1), we use a sigmoid output at the last layer to estimate the chance of the output being 1, corresponding to the user having just said \"activate.\"\n",
    "\n",
    "Note that we use a uni-directional RNN rather than a bi-directional RNN. This is really important for trigger word detection, since we want to be able to detect the trigger word almost immediately after it is said. If we used a bi-directional RNN, we would have to wait for the whole 10 sec of audio to be recorded before we could tell if \"activate\" was said in the first second of the audio clip.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trigger_model:\n",
    "    \n",
    "    def __init__(self, batch_size, Tx, n_freq, Ty, gru_size, gru_layers, learning_rate, decay_rate):\n",
    "    \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs_, self.targets_, self.training, self.keep_prob = build_inputs(Tx, n_freq, Ty)\n",
    "        \n",
    "        # CONV layer\n",
    "        self.conv1d_output=tf.layers.conv1d(self.inputs_,filters=256,kernel_size=15,strides=4)\n",
    "        self.batchNorm_ = tf.layers.batch_normalization(self.conv1d_output, training=self.training)\n",
    "        self.batchNorm_act = tf.nn.relu(self.batchNorm_)\n",
    "        self.dropout_ = tf.nn.dropout(self.batchNorm_act,rate=1-self.keep_prob)\n",
    "\n",
    "         # First GRU Layer\n",
    "        with tf.variable_scope('GRU_1'):\n",
    "            gru_cell_1, self.initial_state_gru_cell_1 = build_gru(gru_size, gru_layers, batch_size, self.keep_prob)\n",
    "            self.outputs_gru_cell_1, state_1 = tf.nn.dynamic_rnn(gru_cell_1, self.dropout_, initial_state=self.initial_state_gru_cell_1)\n",
    "            self.final_state_gru_cell_1 = state_1\n",
    "            self.batchNorm_1 = tf.layers.batch_normalization(self.outputs_gru_cell_1, training=self.training)\n",
    "        \n",
    "         # Second GRU Layer\n",
    "        with tf.variable_scope('GRU_2'):\n",
    "            gru_cell_2, self.initial_state_gru_cell_2 = build_gru(gru_size, gru_layers, batch_size, self.keep_prob)\n",
    "            self.outputs_gru_cell_2, state_2 = tf.nn.dynamic_rnn(gru_cell_2, self.batchNorm_1, initial_state=self.initial_state_gru_cell_2)\n",
    "            self.final_state_gru_cell_2 = state_2\n",
    "            self.batchNorm_2 = tf.layers.batch_normalization(self.outputs_gru_cell_2, training=self.training)\n",
    "            self.dropout_2 = tf.nn.dropout(self.batchNorm_2,rate=0.8)\n",
    "        \n",
    "         # Dense layer with sigmoid activation\n",
    "            self.logits = tf.contrib.layers.fully_connected(self.dropout_2, 1, activation_fn=None, scope='logits')\n",
    "            self.predictions = tf.nn.sigmoid(self.logits) \n",
    "        \n",
    "#         correct_pred = tf.equal(tf.round(self.predictions), self.targets_)\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "        self.loss = build_loss(self.logits, self.targets_)\n",
    "        self.optimizer, self.learning_rate = build_optimizer(self.loss, learning_rate, decay_rate)\n",
    "        \n",
    "        self.acc, self.acc_op = tf.metrics.accuracy(labels=self.targets_, \n",
    "                                  predictions=tf.round(self.predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram\n",
    "Ty = 1375 # The number of time steps in the output of our model\n",
    "batch_size = 5\n",
    "gru_size = 128 \n",
    "gru_layers = 1\n",
    "learning_rate = 0.0001\n",
    "decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weights (trained in keras) from the hd5 file and create a checkpoint\n",
    "### run the next cell 1 time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 21:38:31.580236 4541654464 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 21:38:31.840733 4541654464 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 21:38:32.149090 4541654464 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 21:38:32.278696 4541654464 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0815 21:38:33.092138 4541654464 deprecation.py:323] From /Users/vasileios-mariosgkortsas/anaconda3/envs/dog-project/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model_pretrained=tf.keras.models.load_model('./models/tr_model.h5')\n",
    "sess = tf.keras.backend.get_session()\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"checkpoints_pretrained/model_pretrained.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrained weights from hd5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0815 21:38:46.105550 4541654464 deprecation.py:323] From /Users/vasileios-mariosgkortsas/anaconda3/envs/dog-project/lib/python3.5/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d_3/kernel:0 (15, 101, 256)\n",
      "conv1d_3/bias:0 (256,)\n",
      "batch_normalization_7/gamma:0 (256,)\n",
      "batch_normalization_7/beta:0 (256,)\n",
      "gru_5/kernel:0 (256, 384)\n",
      "gru_5/recurrent_kernel:0 (128, 384)\n",
      "gru_5/bias:0 (384,)\n",
      "batch_normalization_8/gamma:0 (128,)\n",
      "batch_normalization_8/beta:0 (128,)\n",
      "gru_6/kernel:0 (128, 384)\n",
      "gru_6/recurrent_kernel:0 (128, 384)\n",
      "gru_6/bias:0 (384,)\n",
      "batch_normalization_9/gamma:0 (128,)\n",
      "batch_normalization_9/beta:0 (128,)\n",
      "time_distributed_3/kernel:0 (128, 1)\n",
      "time_distributed_3/bias:0 (1,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph('checkpoints_pretrained/model_pretrained.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('checkpoints_pretrained'))    \n",
    "    for variable in tf.trainable_variables():\n",
    "        print(variable.name, variable.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign the pretrained weight values to variables that we will use to load the trainable weights of the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the pretrained weight values to variables that we will use to load the trainable weights of my model\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.import_meta_graph('checkpoints_pretrained/model_pretrained.ckpt.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('checkpoints_pretrained'))\n",
    "    \n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'conv1d_3/kernel:0']\n",
    "    pretrained_var_1=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'conv1d_3/bias:0']\n",
    "    pretrained_var_2=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'batch_normalization_7/gamma:0']\n",
    "    pretrained_var_3=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'batch_normalization_7/beta:0']\n",
    "    pretrained_var_4=np.array(sess.run(var))\n",
    "    \n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'gru_5/kernel:0']\n",
    "    pretrained_gru1_kernel=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'gru_5/recurrent_kernel:0']\n",
    "    pretrained_gru1_recurrent_kernel=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'gru_5/bias:0']\n",
    "    pretrained_gru1_bias=np.array(sess.run(var))\n",
    "    \n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'batch_normalization_8/gamma:0']\n",
    "    pretrained_var_8=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'batch_normalization_8/beta:0']\n",
    "    pretrained_var_9=np.array(sess.run(var))\n",
    "    \n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'gru_6/kernel:0']\n",
    "    pretrained_gru2_kernel=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'gru_6/recurrent_kernel:0']\n",
    "    pretrained_gru2_recurrent_kernel=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'gru_6/bias:0']\n",
    "    pretrained_gru2_bias=np.array(sess.run(var))\n",
    "    \n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'batch_normalization_9/gamma:0']\n",
    "    pretrained_var_13=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'batch_normalization_9/beta:0']\n",
    "    pretrained_var_14=np.array(sess.run(var))\n",
    "    \n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'time_distributed_3/kernel:0']\n",
    "    pretrained_var_15=np.array(sess.run(var))\n",
    "    var = [v for v in tf.trainable_variables() if v.name == 'time_distributed_3/bias:0']\n",
    "    pretrained_var_16=np.array(sess.run(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable parameters of the TesnorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d/kernel:0 (15, 101, 256)\n",
      "conv1d/bias:0 (256,)\n",
      "batch_normalization/gamma:0 (256,)\n",
      "batch_normalization/beta:0 (256,)\n",
      "GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0 (384, 256)\n",
      "GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0 (256,)\n",
      "GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0 (384, 128)\n",
      "GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0 (128,)\n",
      "GRU_1/batch_normalization/gamma:0 (128,)\n",
      "GRU_1/batch_normalization/beta:0 (128,)\n",
      "GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0 (256, 256)\n",
      "GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0 (256,)\n",
      "GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0 (256, 128)\n",
      "GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0 (128,)\n",
      "GRU_2/batch_normalization/gamma:0 (128,)\n",
      "GRU_2/batch_normalization/beta:0 (128,)\n",
      "GRU_2/logits/weights:0 (128, 1)\n",
      "GRU_2/logits/biases:0 (1,)\n"
     ]
    }
   ],
   "source": [
    "# trainable parameters of my model\n",
    "tf.reset_default_graph()\n",
    "model = trigger_model(batch_size=batch_size, Tx=Tx, n_freq=n_freq, Ty=Ty, gru_size=gru_size, gru_layers=gru_layers,\n",
    "                     learning_rate=learning_rate, decay_rate=decay_rate)\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for variable in tf.trainable_variables():\n",
    "        print(variable.name, variable.shape)\n",
    "        #print(sess.run(variable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Iteration: 1 Train loss: 0.075 Train accuracy: 0.9811\n",
      "Epoch: 1/1 Iteration: 2 Train loss: 0.065 Train accuracy: 0.9799\n",
      "Epoch: 1/1 Iteration: 3 Train loss: 0.066 Train accuracy: 0.9792\n",
      "Epoch: 1/1 Iteration: 4 Train loss: 0.090 Train accuracy: 0.9755\n",
      "Epoch: 1/1 Iteration: 5 Train loss: 0.254 Train accuracy: 0.9672\n",
      "Train accuracy (mean): 0.9766\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = trigger_model(batch_size=batch_size, Tx=Tx, n_freq=n_freq, Ty=Ty, gru_size=gru_size, gru_layers=gru_layers,\n",
    "                     learning_rate=learning_rate, decay_rate=decay_rate)\n",
    "extra_graphkeys_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "epochs = 1\n",
    "saver = tf.train.Saver() \n",
    "\n",
    "train_acc = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # assign to my trainable weights the pretrained weight values\n",
    "    for variable in tf.trainable_variables():\n",
    "\n",
    "        if (variable.name=='conv1d/kernel:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_1[0]))\n",
    "        elif (variable.name=='conv1d/bias:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_2[0]))\n",
    "        elif (variable.name=='batch_normalization/gamma:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_3[0]))\n",
    "        elif (variable.name=='batch_normalization/beta:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_4[0]))\n",
    "            \n",
    "        elif (variable.name=='GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0'):\n",
    "            gate_kernel, _=gate_and_candidate_kernel(pretrained_gru1_kernel,\n",
    "                                                                    pretrained_gru1_recurrent_kernel)\n",
    "            sess.run(tf.assign(variable, gate_kernel))\n",
    "        elif (variable.name=='GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0'):\n",
    "            _, candidate_kernel=gate_and_candidate_kernel(pretrained_gru1_kernel,\n",
    "                                                                    pretrained_gru1_recurrent_kernel)\n",
    "            sess.run(tf.assign(variable, candidate_kernel))\n",
    "        elif (variable.name=='GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0'):\n",
    "            bias_1,_=convert_bias(pretrained_gru1_bias[0])\n",
    "            sess.run(tf.assign(variable, bias_1))\n",
    "        elif (variable.name=='GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0'):\n",
    "            _,bias_2=convert_bias(pretrained_gru1_bias[0])\n",
    "            sess.run(tf.assign(variable, bias_2))\n",
    "            \n",
    "        elif (variable.name=='GRU_1/batch_normalization/gamma:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_8[0]))\n",
    "        elif (variable.name=='GRU_1/batch_normalization/beta:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_9[0]))\n",
    "            \n",
    "        elif (variable.name=='GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0'):\n",
    "            gate_kernel, _=gate_and_candidate_kernel(pretrained_gru2_kernel,\n",
    "                                                                    pretrained_gru2_recurrent_kernel)\n",
    "            sess.run(tf.assign(variable, gate_kernel))\n",
    "        elif (variable.name=='GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0'):\n",
    "            _, candidate_kernel=gate_and_candidate_kernel(pretrained_gru2_kernel,\n",
    "                                                                    pretrained_gru2_recurrent_kernel)\n",
    "            sess.run(tf.assign(variable, candidate_kernel))\n",
    "        elif (variable.name=='GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0'):\n",
    "            bias_1,_=convert_bias(pretrained_gru2_bias[0])\n",
    "            sess.run(tf.assign(variable, bias_1))\n",
    "        elif (variable.name=='GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0'):\n",
    "            _,bias_2=convert_bias(pretrained_gru2_bias[0])\n",
    "            sess.run(tf.assign(variable, bias_2))\n",
    "            \n",
    "        elif (variable.name=='GRU_2/batch_normalization/gamma:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_13[0]))\n",
    "        elif (variable.name=='GRU_2/batch_normalization/beta:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_14[0]))\n",
    "            \n",
    "        elif (variable.name=='GRU_2/logits/weights:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_15[0]))\n",
    "        elif (variable.name=='GRU_2/logits/biases:0'):\n",
    "            sess.run(tf.assign(variable, pretrained_var_16[0]))\n",
    "            \n",
    "            \n",
    "    for e in range(epochs):\n",
    "        iteration = 1\n",
    "        state_gru_cell_1 = sess.run(model.initial_state_gru_cell_1) \n",
    "        state_gru_cell_2 = sess.run(model.initial_state_gru_cell_2)\n",
    "        \n",
    "        for (x, y) in get_batches(X, Y, batch_size):\n",
    "            feed = {model.inputs_: x,\n",
    "                     model.targets_: y,\n",
    "                     model.keep_prob: 0.2,\n",
    "                     model.training: True,\n",
    "                     model.initial_state_gru_cell_1: state_gru_cell_1,\n",
    "                     model.initial_state_gru_cell_2: state_gru_cell_2}\n",
    "      \n",
    "            batch_loss, batch_acc, batch_acc_op,state_gru_cell_2,state_gru_cell_1,_,_=sess.run([model.loss,\n",
    "                                                model.acc, model.acc_op,\n",
    "                                                model.final_state_gru_cell_2,\n",
    "                                                model.final_state_gru_cell_1,\n",
    "                                                model.optimizer, \n",
    "                                                 extra_graphkeys_update_ops],feed_dict=feed)\n",
    "            \n",
    "            train_acc.append(batch_acc_op)\n",
    "            \n",
    "            if iteration%1==0:\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(batch_loss),\n",
    "                      \"Train accuracy: {:.4f}\".format(batch_acc_op)\n",
    "                     )\n",
    "                \n",
    "            iteration +=1\n",
    "            \n",
    "    print(\"Train accuracy (mean): {:.4f}\".format(np.mean(train_acc)))\n",
    "    saver.save(sess, \"checkpoints/trigger_word_detection.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to confirm that variables are not trained during testing phase, get tha velaue of trainable variable\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = trigger_model(batch_size=batch_size, Tx=Tx, n_freq=n_freq, Ty=Ty, gru_size=gru_size, gru_layers=gru_layers,\n",
    "                     learning_rate=learning_rate, decay_rate=decay_rate)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "\n",
    "    for variable in tf.trainable_variables():\n",
    "        if (variable.name=='GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0'):\n",
    "            train_var=sess.run(variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Test loss: 0.699 Test accuracy: 0.9271\n",
      "Iteration: 2 Test loss: 0.643 Test accuracy: 0.9306\n",
      "Iteration: 3 Test loss: 0.649 Test accuracy: 0.9318\n",
      "Iteration: 4 Test loss: 0.702 Test accuracy: 0.9305\n",
      "Iteration: 5 Test loss: 0.634 Test accuracy: 0.9313\n",
      "Test accuracy (mean): 0.9303\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = trigger_model(batch_size=batch_size, Tx=Tx, n_freq=n_freq, Ty=Ty, gru_size=gru_size, gru_layers=gru_layers,\n",
    "                     learning_rate=learning_rate, decay_rate=decay_rate)\n",
    "\n",
    "\n",
    "test_acc=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    state_gru_cell_1 = sess.run(model.initial_state_gru_cell_1) \n",
    "    state_gru_cell_2 = sess.run(model.initial_state_gru_cell_2)\n",
    "    \n",
    "    iteration=1   \n",
    "    for (x, y) in get_batches(X_dev, Y_dev, batch_size):\n",
    "        feed = {model.inputs_: x,\n",
    "                     model.targets_: y,\n",
    "                     model.keep_prob: 1,\n",
    "                     model.training: False,\n",
    "                     model.initial_state_gru_cell_1: state_gru_cell_1,\n",
    "                     model.initial_state_gru_cell_2: state_gru_cell_2}\n",
    "      \n",
    "        batch_loss, batch_acc, batch_acc_op,state_gru_cell_2,state_gru_cell_1=sess.run([model.loss,\n",
    "                                                model.acc, model.acc_op,\n",
    "                                                model.final_state_gru_cell_2,\n",
    "                                                model.final_state_gru_cell_1],feed_dict=feed)\n",
    "        \n",
    "        if iteration%1==0:\n",
    "            print(\"Iteration: {}\".format(iteration),\n",
    "                      \"Test loss: {:.3f}\".format(batch_loss),\n",
    "                      \"Test accuracy: {:.4f}\".format(batch_acc_op)\n",
    "                     )\n",
    "        iteration +=1\n",
    "\n",
    "        test_acc.append(batch_acc_op)\n",
    "    print(\"Test accuracy (mean): {:.4f}\".format(np.mean(test_acc)))\n",
    "    \n",
    "    # to confirm that variables are not trained during testing phase\n",
    "    for variable in tf.trainable_variables():\n",
    "        if (variable.name=='GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0'):\n",
    "            test_var=sess.run(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to confirm that variables are not trained during testing phase\n",
    "np.array_equal(train_var,test_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conv1d/kernel:0' shape=(15, 101, 256) dtype=float32_ref>\n",
      "shape of weight matrix:  (15, 101, 256)\n",
      "number of trainable parameters:  387840\n",
      "------------------------------\n",
      "<tf.Variable 'conv1d/bias:0' shape=(256,) dtype=float32_ref>\n",
      "shape of weight matrix:  (256,)\n",
      "number of trainable parameters:  256\n",
      "------------------------------\n",
      "<tf.Variable 'batch_normalization/gamma:0' shape=(256,) dtype=float32_ref>\n",
      "shape of weight matrix:  (256,)\n",
      "number of trainable parameters:  256\n",
      "------------------------------\n",
      "<tf.Variable 'batch_normalization/beta:0' shape=(256,) dtype=float32_ref>\n",
      "shape of weight matrix:  (256,)\n",
      "number of trainable parameters:  256\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0' shape=(384, 256) dtype=float32_ref>\n",
      "shape of weight matrix:  (384, 256)\n",
      "number of trainable parameters:  98304\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0' shape=(256,) dtype=float32_ref>\n",
      "shape of weight matrix:  (256,)\n",
      "number of trainable parameters:  256\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0' shape=(384, 128) dtype=float32_ref>\n",
      "shape of weight matrix:  (384, 128)\n",
      "number of trainable parameters:  49152\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_1/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0' shape=(128,) dtype=float32_ref>\n",
      "shape of weight matrix:  (128,)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_1/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>\n",
      "shape of weight matrix:  (128,)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_1/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>\n",
      "shape of weight matrix:  (128,)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "shape of weight matrix:  (256, 256)\n",
      "number of trainable parameters:  65536\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/gates/bias:0' shape=(256,) dtype=float32_ref>\n",
      "shape of weight matrix:  (256,)\n",
      "number of trainable parameters:  256\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0' shape=(256, 128) dtype=float32_ref>\n",
      "shape of weight matrix:  (256, 128)\n",
      "number of trainable parameters:  32768\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/rnn/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0' shape=(128,) dtype=float32_ref>\n",
      "shape of weight matrix:  (128,)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>\n",
      "shape of weight matrix:  (128,)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>\n",
      "shape of weight matrix:  (128,)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/logits/weights:0' shape=(128, 1) dtype=float32_ref>\n",
      "shape of weight matrix:  (128, 1)\n",
      "number of trainable parameters:  128\n",
      "------------------------------\n",
      "<tf.Variable 'GRU_2/logits/biases:0' shape=(1,) dtype=float32_ref>\n",
      "shape of weight matrix:  (1,)\n",
      "number of trainable parameters:  1\n",
      "------------------------------\n",
      "total number of trainable parameters:  635777\n"
     ]
    }
   ],
   "source": [
    "model = trigger_model(batch_size=batch_size, Tx=Tx, n_freq=n_freq, Ty=Ty, gru_size=gru_size, gru_layers=gru_layers,\n",
    "                     learning_rate=learning_rate, decay_rate=decay_rate)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        print(variable)\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        print('shape of weight matrix: ',shape)\n",
    "        #print(len(shape))\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            #print(dim)\n",
    "            variable_parameters *= dim.value\n",
    "        print('number of trainable parameters: ',variable_parameters)\n",
    "        print('------------------------------')\n",
    "        total_parameters += variable_parameters\n",
    "    print('total number of trainable parameters: ',total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "rSupZ",
   "launcher_item_id": "cvGhe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
