{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Character level text generation using LSTM\n",
    "\n",
    "This notebook is based on the programming assignment of deeplearning.ai, course Sequence Models, week Recurrent Neural Networks. The figures are also taken from that assignment.\n",
    "\n",
    "We will build a character-level text generation LSTM network to generate new dinosaur names.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "- Application of long short-term memory (LSTM) neural network in TensorFlow\n",
    "- Use of Adam optimizer \n",
    "- Use gradient clipping\n",
    "- Use variable length sequences in TensorFlow\n",
    "- Apply padding to make all sequences have the same length\n",
    "- Use of GPU for the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The characters are a-z (26 characters) plus the \"\\n\" (or newline character), which in this assignment plays a role similar to the `<EOS>` (or \"End of sentence\") token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, `char_to_ix` and `ix_to_char` are the python dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "{'b': 2, 'e': 5, 'q': 17, 'u': 21, 'l': 12, 'r': 18, 'z': 26, 'c': 3, 'k': 11, 'g': 7, 't': 20, 'w': 23, 'y': 25, 'f': 6, 'h': 8, 'm': 13, '\\n': 0, 's': 19, 'j': 10, 'x': 24, 'i': 9, 'p': 16, 'd': 4, 'n': 14, 'a': 1, 'v': 22, 'o': 15}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aachenosaurus',\n",
       " 'aardonyx',\n",
       " 'abdallahsaurus',\n",
       " 'abelisaurus',\n",
       " 'abrictosaurus',\n",
       " 'abrosaurus',\n",
       " 'abydosaurus',\n",
       " 'acanthopholis',\n",
       " 'achelousaurus',\n",
       " 'acheroraptor']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = data.split('\\n')\n",
    "names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "convert each name to list of indices, by converting each character to the corresponding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "names_ints = []\n",
    "for each in names:\n",
    "    names_ints.append([char_to_ix[word] for word in each])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum name length: 26\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "names_lens = Counter([len(x) for x in names_ints])\n",
    "print(\"Maximum name length: {}\".format(max(names_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 1,\n",
       "         4: 5,\n",
       "         5: 7,\n",
       "         6: 13,\n",
       "         7: 22,\n",
       "         8: 73,\n",
       "         9: 94,\n",
       "         10: 191,\n",
       "         11: 232,\n",
       "         12: 268,\n",
       "         13: 269,\n",
       "         14: 148,\n",
       "         15: 104,\n",
       "         16: 64,\n",
       "         17: 22,\n",
       "         18: 13,\n",
       "         19: 3,\n",
       "         20: 2,\n",
       "         21: 1,\n",
       "         23: 2,\n",
       "         25: 1,\n",
       "         26: 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TensorFlow unfolds our recurrent network for a given number of steps. Assuming each character of each name corresponds to one time step, each input, even within the same batch, will have variable lengths. Thus, we need to specify a maximum sequence length and pad all shorter sequences.\n",
    "\n",
    "Since the dinosaur names have variable lengths we will pad shorter names, so all names have same lengths. We will pad the missing characters with -1. We pad with -1, because in this case tf.one_hot(self.inputs, num_classes) will give 0s for the -1. We want to use one-hot vectors for the actual characters    \n",
    "\n",
    "The longest dinosaur name is 26 characters (i.e. time steps). I set the recurrent network to have 30 time steps (seq_len=30)\n",
    "\n",
    "The first entry of features_x being -1 will be interpreted (when we do one-hot encoding) as setting  $x^{\\langle 0 \\rangle} = \\vec{0}$. Further, this ensures that features_y is equal to features_x but shifted one step to the left, and with an additional \"\\n\" appended to signify the end of the dinosaur name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seq_len=30\n",
    "features_x = np.ones((len(names_ints), seq_len), dtype=int)*(-1)\n",
    "features_y = np.ones((len(names_ints), seq_len), dtype=int)*(-1)\n",
    "for i, row in enumerate(names_ints):\n",
    "    features_x[i, 1:len(row)+1] = np.array(row) # the first entry in each row of features_x is -1  \n",
    "    features_y[i, :len(row)] = np.array(row)\n",
    "    features_y[i, len(row):len(row)+1]=char_to_ix['\\n']\n",
    "    # features_y is equal to features_x but shifted one step \n",
    "    # to the left, and with an additional \"\\n\" appended to signify the end of the dinosaur name\n",
    "    \n",
    "    # I had before features_x[i, 1:len(row)+1] = np.array(row)[:seq_len] and features_y[i, :len(row)] = np.array(row)[:seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "code taken from https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "\n",
    "We compute the sequence lengths (lengths of input dinosaur names wihtout padding). We assume that the sequences are padded with -1 to fill up the remaining time steps in the batch\n",
    "\n",
    "We want to know the length (without padding) of the input sequence, so that the output of the LSTM cells corresponding to the padded time steps is 0. Since, the first character of the input is set to -1, we need to add an 1 to the line length = tf.reduce_sum(used, 1)+1\n",
    "\n",
    "The function returns a vector of size batch_size, which gives the length for each sequence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def length(sequence):   \n",
    "  used = tf.sign(tf.reduce_max(tf.abs(sequence), 2))\n",
    "  length = tf.reduce_sum(used, 1)+1 #we add 1 to account for the fact that we start with a zeroth pad\n",
    "  length = tf.cast(length, tf.int32)\n",
    "  return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_steps],name='inputs')\n",
    "    targets = tf.placeholder(tf.int32,[batch_size,num_steps], name='targets')\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, lstm_layers, batch_size):\n",
    "    \n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size) \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    \n",
    "    seq_output = tf.concat(lstm_output,axis=1)\n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    x = tf.reshape(seq_output,[-1,lstm_size]) # i think lstm_size should be replaced with in_size\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Create the weight and bias variables here\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size]))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.add(tf.matmul(x,softmax_w),softmax_b)\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    predictions = tf.nn.softmax(logits)     \n",
    "\n",
    "    return predictions, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    \n",
    "    y_one_hot = tf.one_hot(targets, depth=num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot,[-1,num_classes])\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped))\n",
    "    \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharLSTM:\n",
    "    \n",
    "    def __init__(self, num_classes=vocab_size, batch_size=10, num_steps=seq_len, \n",
    "                       lstm_size=128, lstm_layers=1, learning_rate=0.01, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, lstm_layers, batch_size)\n",
    "            \n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        inputs_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "                \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell ,inputs_one_hot, sequence_length=length(inputs_one_hot), \n",
    "                                           initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "\n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss =  build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10        # Sequences per batch\n",
    "num_steps = seq_len    # Number of sequence steps per batch\n",
    "lstm_size = 128         # Size of hidden layers in LSTMs\n",
    "lstm_layers = 1          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "grad_clip=5\n",
    "num_classes=vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0815 04:06:33.783807 33208 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0815 04:06:33.784775 33208 deprecation.py:323] From <ipython-input-17-21b64da84c03>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0815 04:06:33.784775 33208 deprecation.py:323] From <ipython-input-17-21b64da84c03>:7: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0815 04:06:33.803759 33208 deprecation.py:323] From <ipython-input-21-cb0911ddc517>:28: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0815 04:06:34.031145 33208 deprecation.py:506] From C:\\Users\\vgkortsas\\AppData\\Local\\Continuum\\anaconda3\\envs\\TF_practice\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0815 04:06:34.040123 33208 deprecation.py:506] From C:\\Users\\vgkortsas\\AppData\\Local\\Continuum\\anaconda3\\envs\\TF_practice\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0815 04:06:34.301392 33208 deprecation.py:323] From C:\\Users\\vgkortsas\\AppData\\Local\\Continuum\\anaconda3\\envs\\TF_practice\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0815 04:06:34.332342 33208 deprecation.py:323] From <ipython-input-19-f3c928699157>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/230 Iteration: 0 Train loss: 1.411\n",
      "Epoch: 1/230 Iteration: 50 Train loss: 1.246\n",
      "Epoch: 1/230 Iteration: 100 Train loss: 1.162\n",
      "Epoch: 1/230 Iteration: 150 Train loss: 0.911\n",
      "Epoch: 2/230 Iteration: 200 Train loss: 0.847\n",
      "Epoch: 2/230 Iteration: 250 Train loss: 0.905\n",
      "Epoch: 2/230 Iteration: 300 Train loss: 0.842\n",
      "Epoch: 3/230 Iteration: 350 Train loss: 0.600\n",
      "Epoch: 3/230 Iteration: 400 Train loss: 0.749\n",
      "Epoch: 3/230 Iteration: 450 Train loss: 0.812\n",
      "Epoch: 4/230 Iteration: 500 Train loss: 0.824\n",
      "Epoch: 4/230 Iteration: 550 Train loss: 0.633\n",
      "Epoch: 4/230 Iteration: 600 Train loss: 0.686\n",
      "Epoch: 5/230 Iteration: 650 Train loss: 0.728\n",
      "Epoch: 5/230 Iteration: 700 Train loss: 0.643\n",
      "Epoch: 5/230 Iteration: 750 Train loss: 0.518\n",
      "Epoch: 6/230 Iteration: 800 Train loss: 0.588\n",
      "Epoch: 6/230 Iteration: 850 Train loss: 0.697\n",
      "Epoch: 6/230 Iteration: 900 Train loss: 0.560\n",
      "Epoch: 7/230 Iteration: 950 Train loss: 0.615\n",
      "Epoch: 7/230 Iteration: 1000 Train loss: 0.693\n",
      "Epoch: 7/230 Iteration: 1050 Train loss: 0.644\n",
      "Epoch: 8/230 Iteration: 1100 Train loss: 0.710\n",
      "Epoch: 8/230 Iteration: 1150 Train loss: 0.664\n",
      "Epoch: 8/230 Iteration: 1200 Train loss: 0.674\n",
      "Epoch: 9/230 Iteration: 1250 Train loss: 0.552\n",
      "Epoch: 9/230 Iteration: 1300 Train loss: 0.547\n",
      "Epoch: 9/230 Iteration: 1350 Train loss: 0.580\n",
      "Epoch: 10/230 Iteration: 1400 Train loss: 0.629\n",
      "Epoch: 10/230 Iteration: 1450 Train loss: 0.530\n",
      "Epoch: 10/230 Iteration: 1500 Train loss: 0.471\n",
      "Epoch: 11/230 Iteration: 1550 Train loss: 0.716\n",
      "Epoch: 11/230 Iteration: 1600 Train loss: 0.634\n",
      "Epoch: 11/230 Iteration: 1650 Train loss: 0.642\n",
      "Epoch: 12/230 Iteration: 1700 Train loss: 0.482\n",
      "Epoch: 12/230 Iteration: 1750 Train loss: 0.504\n",
      "Epoch: 12/230 Iteration: 1800 Train loss: 0.495\n",
      "Epoch: 13/230 Iteration: 1850 Train loss: 0.508\n",
      "Epoch: 13/230 Iteration: 1900 Train loss: 0.495\n",
      "Epoch: 13/230 Iteration: 1950 Train loss: 0.479\n",
      "Epoch: 14/230 Iteration: 2000 Train loss: 0.386\n",
      "Epoch: 14/230 Iteration: 2050 Train loss: 0.570\n",
      "Epoch: 14/230 Iteration: 2100 Train loss: 0.427\n",
      "Epoch: 15/230 Iteration: 2150 Train loss: 0.474\n",
      "Epoch: 15/230 Iteration: 2200 Train loss: 0.462\n",
      "Epoch: 15/230 Iteration: 2250 Train loss: 0.415\n",
      "Epoch: 16/230 Iteration: 2300 Train loss: 0.487\n",
      "Epoch: 16/230 Iteration: 2350 Train loss: 0.365\n",
      "Epoch: 16/230 Iteration: 2400 Train loss: 0.450\n",
      "Epoch: 17/230 Iteration: 2450 Train loss: 0.475\n",
      "Epoch: 17/230 Iteration: 2500 Train loss: 0.399\n",
      "Epoch: 17/230 Iteration: 2550 Train loss: 0.423\n",
      "Epoch: 17/230 Iteration: 2600 Train loss: 0.331\n",
      "Epoch: 18/230 Iteration: 2650 Train loss: 0.340\n",
      "Epoch: 18/230 Iteration: 2700 Train loss: 0.352\n",
      "Epoch: 18/230 Iteration: 2750 Train loss: 0.358\n",
      "Epoch: 19/230 Iteration: 2800 Train loss: 0.459\n",
      "Epoch: 19/230 Iteration: 2850 Train loss: 0.528\n",
      "Epoch: 19/230 Iteration: 2900 Train loss: 0.402\n",
      "Epoch: 20/230 Iteration: 2950 Train loss: 0.449\n",
      "Epoch: 20/230 Iteration: 3000 Train loss: 0.357\n",
      "Epoch: 20/230 Iteration: 3050 Train loss: 0.387\n",
      "Epoch: 21/230 Iteration: 3100 Train loss: 0.338\n",
      "Epoch: 21/230 Iteration: 3150 Train loss: 0.410\n",
      "Epoch: 21/230 Iteration: 3200 Train loss: 0.354\n",
      "Epoch: 22/230 Iteration: 3250 Train loss: 0.388\n",
      "Epoch: 22/230 Iteration: 3300 Train loss: 0.322\n",
      "Epoch: 22/230 Iteration: 3350 Train loss: 0.319\n",
      "Epoch: 23/230 Iteration: 3400 Train loss: 0.354\n",
      "Epoch: 23/230 Iteration: 3450 Train loss: 0.343\n",
      "Epoch: 23/230 Iteration: 3500 Train loss: 0.400\n",
      "Epoch: 24/230 Iteration: 3550 Train loss: 0.365\n",
      "Epoch: 24/230 Iteration: 3600 Train loss: 0.389\n",
      "Epoch: 24/230 Iteration: 3650 Train loss: 0.404\n",
      "Epoch: 25/230 Iteration: 3700 Train loss: 0.361\n",
      "Epoch: 25/230 Iteration: 3750 Train loss: 0.370\n",
      "Epoch: 25/230 Iteration: 3800 Train loss: 0.350\n",
      "Epoch: 26/230 Iteration: 3850 Train loss: 0.397\n",
      "Epoch: 26/230 Iteration: 3900 Train loss: 0.396\n",
      "Epoch: 26/230 Iteration: 3950 Train loss: 0.259\n",
      "Epoch: 27/230 Iteration: 4000 Train loss: 0.343\n",
      "Epoch: 27/230 Iteration: 4050 Train loss: 0.335\n",
      "Epoch: 27/230 Iteration: 4100 Train loss: 0.328\n",
      "Epoch: 28/230 Iteration: 4150 Train loss: 0.329\n",
      "Epoch: 28/230 Iteration: 4200 Train loss: 0.359\n",
      "Epoch: 28/230 Iteration: 4250 Train loss: 0.287\n",
      "Epoch: 29/230 Iteration: 4300 Train loss: 0.314\n",
      "Epoch: 29/230 Iteration: 4350 Train loss: 0.303\n",
      "Epoch: 29/230 Iteration: 4400 Train loss: 0.333\n",
      "Epoch: 30/230 Iteration: 4450 Train loss: 0.342\n",
      "Epoch: 30/230 Iteration: 4500 Train loss: 0.365\n",
      "Epoch: 30/230 Iteration: 4550 Train loss: 0.321\n",
      "Epoch: 31/230 Iteration: 4600 Train loss: 0.360\n",
      "Epoch: 31/230 Iteration: 4650 Train loss: 0.339\n",
      "Epoch: 31/230 Iteration: 4700 Train loss: 0.319\n",
      "Epoch: 32/230 Iteration: 4750 Train loss: 0.300\n",
      "Epoch: 32/230 Iteration: 4800 Train loss: 0.293\n",
      "Epoch: 32/230 Iteration: 4850 Train loss: 0.273\n",
      "Epoch: 33/230 Iteration: 4900 Train loss: 0.271\n",
      "Epoch: 33/230 Iteration: 4950 Train loss: 0.330\n",
      "Epoch: 33/230 Iteration: 5000 Train loss: 0.311\n",
      "Epoch: 34/230 Iteration: 5050 Train loss: 0.327\n",
      "Epoch: 34/230 Iteration: 5100 Train loss: 0.299\n",
      "Epoch: 34/230 Iteration: 5150 Train loss: 0.331\n",
      "Epoch: 34/230 Iteration: 5200 Train loss: 0.295\n",
      "Epoch: 35/230 Iteration: 5250 Train loss: 0.360\n",
      "Epoch: 35/230 Iteration: 5300 Train loss: 0.285\n",
      "Epoch: 35/230 Iteration: 5350 Train loss: 0.274\n",
      "Epoch: 36/230 Iteration: 5400 Train loss: 0.288\n",
      "Epoch: 36/230 Iteration: 5450 Train loss: 0.330\n",
      "Epoch: 36/230 Iteration: 5500 Train loss: 0.342\n",
      "Epoch: 37/230 Iteration: 5550 Train loss: 0.284\n",
      "Epoch: 37/230 Iteration: 5600 Train loss: 0.283\n",
      "Epoch: 37/230 Iteration: 5650 Train loss: 0.314\n",
      "Epoch: 38/230 Iteration: 5700 Train loss: 0.278\n",
      "Epoch: 38/230 Iteration: 5750 Train loss: 0.269\n",
      "Epoch: 38/230 Iteration: 5800 Train loss: 0.326\n",
      "Epoch: 39/230 Iteration: 5850 Train loss: 0.263\n",
      "Epoch: 39/230 Iteration: 5900 Train loss: 0.318\n",
      "Epoch: 39/230 Iteration: 5950 Train loss: 0.277\n",
      "Epoch: 40/230 Iteration: 6000 Train loss: 0.275\n",
      "Epoch: 40/230 Iteration: 6050 Train loss: 0.305\n",
      "Epoch: 40/230 Iteration: 6100 Train loss: 0.270\n",
      "Epoch: 41/230 Iteration: 6150 Train loss: 0.322\n",
      "Epoch: 41/230 Iteration: 6200 Train loss: 0.269\n",
      "Epoch: 41/230 Iteration: 6250 Train loss: 0.272\n",
      "Epoch: 42/230 Iteration: 6300 Train loss: 0.278\n",
      "Epoch: 42/230 Iteration: 6350 Train loss: 0.270\n",
      "Epoch: 42/230 Iteration: 6400 Train loss: 0.268\n",
      "Epoch: 43/230 Iteration: 6450 Train loss: 0.304\n",
      "Epoch: 43/230 Iteration: 6500 Train loss: 0.273\n",
      "Epoch: 43/230 Iteration: 6550 Train loss: 0.297\n",
      "Epoch: 44/230 Iteration: 6600 Train loss: 0.284\n",
      "Epoch: 44/230 Iteration: 6650 Train loss: 0.244\n",
      "Epoch: 44/230 Iteration: 6700 Train loss: 0.284\n",
      "Epoch: 45/230 Iteration: 6750 Train loss: 0.249\n",
      "Epoch: 45/230 Iteration: 6800 Train loss: 0.276\n",
      "Epoch: 45/230 Iteration: 6850 Train loss: 0.321\n",
      "Epoch: 46/230 Iteration: 6900 Train loss: 0.263\n",
      "Epoch: 46/230 Iteration: 6950 Train loss: 0.277\n",
      "Epoch: 46/230 Iteration: 7000 Train loss: 0.318\n",
      "Epoch: 47/230 Iteration: 7050 Train loss: 0.265\n",
      "Epoch: 47/230 Iteration: 7100 Train loss: 0.271\n",
      "Epoch: 47/230 Iteration: 7150 Train loss: 0.243\n",
      "Epoch: 48/230 Iteration: 7200 Train loss: 0.269\n",
      "Epoch: 48/230 Iteration: 7250 Train loss: 0.283\n",
      "Epoch: 48/230 Iteration: 7300 Train loss: 0.268\n",
      "Epoch: 49/230 Iteration: 7350 Train loss: 0.264\n",
      "Epoch: 49/230 Iteration: 7400 Train loss: 0.229\n",
      "Epoch: 49/230 Iteration: 7450 Train loss: 0.346\n",
      "Epoch: 50/230 Iteration: 7500 Train loss: 0.314\n",
      "Epoch: 50/230 Iteration: 7550 Train loss: 0.296\n",
      "Epoch: 50/230 Iteration: 7600 Train loss: 0.282\n",
      "Epoch: 51/230 Iteration: 7650 Train loss: 0.255\n",
      "Epoch: 51/230 Iteration: 7700 Train loss: 0.256\n",
      "Epoch: 51/230 Iteration: 7750 Train loss: 0.262\n",
      "Epoch: 51/230 Iteration: 7800 Train loss: 0.232\n",
      "Epoch: 52/230 Iteration: 7850 Train loss: 0.271\n",
      "Epoch: 52/230 Iteration: 7900 Train loss: 0.253\n",
      "Epoch: 52/230 Iteration: 7950 Train loss: 0.210\n",
      "Epoch: 53/230 Iteration: 8000 Train loss: 0.277\n",
      "Epoch: 53/230 Iteration: 8050 Train loss: 0.308\n",
      "Epoch: 53/230 Iteration: 8100 Train loss: 0.299\n",
      "Epoch: 54/230 Iteration: 8150 Train loss: 0.247\n",
      "Epoch: 54/230 Iteration: 8200 Train loss: 0.281\n",
      "Epoch: 54/230 Iteration: 8250 Train loss: 0.269\n",
      "Epoch: 55/230 Iteration: 8300 Train loss: 0.279\n",
      "Epoch: 55/230 Iteration: 8350 Train loss: 0.260\n",
      "Epoch: 55/230 Iteration: 8400 Train loss: 0.250\n",
      "Epoch: 56/230 Iteration: 8450 Train loss: 0.269\n",
      "Epoch: 56/230 Iteration: 8500 Train loss: 0.282\n",
      "Epoch: 56/230 Iteration: 8550 Train loss: 0.236\n",
      "Epoch: 57/230 Iteration: 8600 Train loss: 0.261\n",
      "Epoch: 57/230 Iteration: 8650 Train loss: 0.257\n",
      "Epoch: 57/230 Iteration: 8700 Train loss: 0.278\n",
      "Epoch: 58/230 Iteration: 8750 Train loss: 0.300\n",
      "Epoch: 58/230 Iteration: 8800 Train loss: 0.327\n",
      "Epoch: 58/230 Iteration: 8850 Train loss: 0.365\n",
      "Epoch: 59/230 Iteration: 8900 Train loss: 0.313\n",
      "Epoch: 59/230 Iteration: 8950 Train loss: 0.261\n",
      "Epoch: 59/230 Iteration: 9000 Train loss: 0.295\n",
      "Epoch: 60/230 Iteration: 9050 Train loss: 0.295\n",
      "Epoch: 60/230 Iteration: 9100 Train loss: 0.332\n",
      "Epoch: 60/230 Iteration: 9150 Train loss: 0.286\n",
      "Epoch: 61/230 Iteration: 9200 Train loss: 0.226\n",
      "Epoch: 61/230 Iteration: 9250 Train loss: 0.299\n",
      "Epoch: 61/230 Iteration: 9300 Train loss: 0.271\n",
      "Epoch: 62/230 Iteration: 9350 Train loss: 0.274\n",
      "Epoch: 62/230 Iteration: 9400 Train loss: 0.287\n",
      "Epoch: 62/230 Iteration: 9450 Train loss: 0.327\n",
      "Epoch: 63/230 Iteration: 9500 Train loss: 0.247\n",
      "Epoch: 63/230 Iteration: 9550 Train loss: 0.243\n",
      "Epoch: 63/230 Iteration: 9600 Train loss: 0.239\n",
      "Epoch: 64/230 Iteration: 9650 Train loss: 0.235\n",
      "Epoch: 64/230 Iteration: 9700 Train loss: 0.218\n",
      "Epoch: 64/230 Iteration: 9750 Train loss: 0.251\n",
      "Epoch: 65/230 Iteration: 9800 Train loss: 0.269\n",
      "Epoch: 65/230 Iteration: 9850 Train loss: 0.242\n",
      "Epoch: 65/230 Iteration: 9900 Train loss: 0.318\n",
      "Epoch: 66/230 Iteration: 9950 Train loss: 0.283\n",
      "Epoch: 66/230 Iteration: 10000 Train loss: 0.239\n",
      "Epoch: 66/230 Iteration: 10050 Train loss: 0.306\n",
      "Epoch: 67/230 Iteration: 10100 Train loss: 0.288\n",
      "Epoch: 67/230 Iteration: 10150 Train loss: 0.258\n",
      "Epoch: 67/230 Iteration: 10200 Train loss: 0.307\n",
      "Epoch: 67/230 Iteration: 10250 Train loss: 0.204\n",
      "Epoch: 68/230 Iteration: 10300 Train loss: 0.301\n",
      "Epoch: 68/230 Iteration: 10350 Train loss: 0.273\n",
      "Epoch: 68/230 Iteration: 10400 Train loss: 0.239\n",
      "Epoch: 69/230 Iteration: 10450 Train loss: 0.268\n",
      "Epoch: 69/230 Iteration: 10500 Train loss: 0.264\n",
      "Epoch: 69/230 Iteration: 10550 Train loss: 0.237\n",
      "Epoch: 70/230 Iteration: 10600 Train loss: 0.310\n",
      "Epoch: 70/230 Iteration: 10650 Train loss: 0.230\n",
      "Epoch: 70/230 Iteration: 10700 Train loss: 0.225\n",
      "Epoch: 71/230 Iteration: 10750 Train loss: 0.212\n",
      "Epoch: 71/230 Iteration: 10800 Train loss: 0.282\n",
      "Epoch: 71/230 Iteration: 10850 Train loss: 0.213\n",
      "Epoch: 72/230 Iteration: 10900 Train loss: 0.249\n",
      "Epoch: 72/230 Iteration: 10950 Train loss: 0.190\n",
      "Epoch: 72/230 Iteration: 11000 Train loss: 0.275\n",
      "Epoch: 73/230 Iteration: 11050 Train loss: 0.269\n",
      "Epoch: 73/230 Iteration: 11100 Train loss: 0.230\n",
      "Epoch: 73/230 Iteration: 11150 Train loss: 0.265\n",
      "Epoch: 74/230 Iteration: 11200 Train loss: 0.284\n",
      "Epoch: 74/230 Iteration: 11250 Train loss: 0.313\n",
      "Epoch: 74/230 Iteration: 11300 Train loss: 0.256\n",
      "Epoch: 75/230 Iteration: 11350 Train loss: 0.270\n",
      "Epoch: 75/230 Iteration: 11400 Train loss: 0.286\n",
      "Epoch: 75/230 Iteration: 11450 Train loss: 0.260\n",
      "Epoch: 76/230 Iteration: 11500 Train loss: 0.274\n",
      "Epoch: 76/230 Iteration: 11550 Train loss: 0.314\n",
      "Epoch: 76/230 Iteration: 11600 Train loss: 0.218\n",
      "Epoch: 77/230 Iteration: 11650 Train loss: 0.264\n",
      "Epoch: 77/230 Iteration: 11700 Train loss: 0.234\n",
      "Epoch: 77/230 Iteration: 11750 Train loss: 0.258\n",
      "Epoch: 78/230 Iteration: 11800 Train loss: 0.249\n",
      "Epoch: 78/230 Iteration: 11850 Train loss: 0.234\n",
      "Epoch: 78/230 Iteration: 11900 Train loss: 0.262\n",
      "Epoch: 79/230 Iteration: 11950 Train loss: 0.234\n",
      "Epoch: 79/230 Iteration: 12000 Train loss: 0.224\n",
      "Epoch: 79/230 Iteration: 12050 Train loss: 0.262\n",
      "Epoch: 80/230 Iteration: 12100 Train loss: 0.292\n",
      "Epoch: 80/230 Iteration: 12150 Train loss: 0.298\n",
      "Epoch: 80/230 Iteration: 12200 Train loss: 0.244\n",
      "Epoch: 81/230 Iteration: 12250 Train loss: 0.255\n",
      "Epoch: 81/230 Iteration: 12300 Train loss: 0.243\n",
      "Epoch: 81/230 Iteration: 12350 Train loss: 0.228\n",
      "Epoch: 82/230 Iteration: 12400 Train loss: 0.254\n",
      "Epoch: 82/230 Iteration: 12450 Train loss: 0.258\n",
      "Epoch: 82/230 Iteration: 12500 Train loss: 0.225\n",
      "Epoch: 83/230 Iteration: 12550 Train loss: 0.251\n",
      "Epoch: 83/230 Iteration: 12600 Train loss: 0.233\n",
      "Epoch: 83/230 Iteration: 12650 Train loss: 0.285\n",
      "Epoch: 84/230 Iteration: 12700 Train loss: 0.303\n",
      "Epoch: 84/230 Iteration: 12750 Train loss: 0.212\n",
      "Epoch: 84/230 Iteration: 12800 Train loss: 0.268\n",
      "Epoch: 84/230 Iteration: 12850 Train loss: 0.241\n",
      "Epoch: 85/230 Iteration: 12900 Train loss: 0.250\n",
      "Epoch: 85/230 Iteration: 12950 Train loss: 0.242\n",
      "Epoch: 85/230 Iteration: 13000 Train loss: 0.232\n",
      "Epoch: 86/230 Iteration: 13050 Train loss: 0.247\n",
      "Epoch: 86/230 Iteration: 13100 Train loss: 0.251\n",
      "Epoch: 86/230 Iteration: 13150 Train loss: 0.263\n",
      "Epoch: 87/230 Iteration: 13200 Train loss: 0.246\n",
      "Epoch: 87/230 Iteration: 13250 Train loss: 0.255\n",
      "Epoch: 87/230 Iteration: 13300 Train loss: 0.221\n",
      "Epoch: 88/230 Iteration: 13350 Train loss: 0.261\n",
      "Epoch: 88/230 Iteration: 13400 Train loss: 0.225\n",
      "Epoch: 88/230 Iteration: 13450 Train loss: 0.255\n",
      "Epoch: 89/230 Iteration: 13500 Train loss: 0.259\n",
      "Epoch: 89/230 Iteration: 13550 Train loss: 0.243\n",
      "Epoch: 89/230 Iteration: 13600 Train loss: 0.243\n",
      "Epoch: 90/230 Iteration: 13650 Train loss: 0.256\n",
      "Epoch: 90/230 Iteration: 13700 Train loss: 0.226\n",
      "Epoch: 90/230 Iteration: 13750 Train loss: 0.217\n",
      "Epoch: 91/230 Iteration: 13800 Train loss: 0.311\n",
      "Epoch: 91/230 Iteration: 13850 Train loss: 0.225\n",
      "Epoch: 91/230 Iteration: 13900 Train loss: 0.218\n",
      "Epoch: 92/230 Iteration: 13950 Train loss: 0.249\n",
      "Epoch: 92/230 Iteration: 14000 Train loss: 0.245\n",
      "Epoch: 92/230 Iteration: 14050 Train loss: 0.203\n",
      "Epoch: 93/230 Iteration: 14100 Train loss: 0.229\n",
      "Epoch: 93/230 Iteration: 14150 Train loss: 0.225\n",
      "Epoch: 93/230 Iteration: 14200 Train loss: 0.250\n",
      "Epoch: 94/230 Iteration: 14250 Train loss: 0.321\n",
      "Epoch: 94/230 Iteration: 14300 Train loss: 0.254\n",
      "Epoch: 94/230 Iteration: 14350 Train loss: 0.200\n",
      "Epoch: 95/230 Iteration: 14400 Train loss: 0.269\n",
      "Epoch: 95/230 Iteration: 14450 Train loss: 0.248\n",
      "Epoch: 95/230 Iteration: 14500 Train loss: 0.271\n",
      "Epoch: 96/230 Iteration: 14550 Train loss: 0.222\n",
      "Epoch: 96/230 Iteration: 14600 Train loss: 0.241\n",
      "Epoch: 96/230 Iteration: 14650 Train loss: 0.254\n",
      "Epoch: 97/230 Iteration: 14700 Train loss: 0.238\n",
      "Epoch: 97/230 Iteration: 14750 Train loss: 0.223\n",
      "Epoch: 97/230 Iteration: 14800 Train loss: 0.234\n",
      "Epoch: 98/230 Iteration: 14850 Train loss: 0.259\n",
      "Epoch: 98/230 Iteration: 14900 Train loss: 0.265\n",
      "Epoch: 98/230 Iteration: 14950 Train loss: 0.186\n",
      "Epoch: 99/230 Iteration: 15000 Train loss: 0.228\n",
      "Epoch: 99/230 Iteration: 15050 Train loss: 0.220\n",
      "Epoch: 99/230 Iteration: 15100 Train loss: 0.240\n",
      "Epoch: 100/230 Iteration: 15150 Train loss: 0.279\n",
      "Epoch: 100/230 Iteration: 15200 Train loss: 0.274\n",
      "Epoch: 100/230 Iteration: 15250 Train loss: 0.200\n",
      "Epoch: 101/230 Iteration: 15300 Train loss: 0.226\n",
      "Epoch: 101/230 Iteration: 15350 Train loss: 0.215\n",
      "Epoch: 101/230 Iteration: 15400 Train loss: 0.247\n",
      "Epoch: 101/230 Iteration: 15450 Train loss: 0.244\n",
      "Epoch: 102/230 Iteration: 15500 Train loss: 0.263\n",
      "Epoch: 102/230 Iteration: 15550 Train loss: 0.273\n",
      "Epoch: 102/230 Iteration: 15600 Train loss: 0.199\n",
      "Epoch: 103/230 Iteration: 15650 Train loss: 0.257\n",
      "Epoch: 103/230 Iteration: 15700 Train loss: 0.285\n",
      "Epoch: 103/230 Iteration: 15750 Train loss: 0.222\n",
      "Epoch: 104/230 Iteration: 15800 Train loss: 0.266\n",
      "Epoch: 104/230 Iteration: 15850 Train loss: 0.272\n",
      "Epoch: 104/230 Iteration: 15900 Train loss: 0.254\n",
      "Epoch: 105/230 Iteration: 15950 Train loss: 0.280\n",
      "Epoch: 105/230 Iteration: 16000 Train loss: 0.242\n",
      "Epoch: 105/230 Iteration: 16050 Train loss: 0.215\n",
      "Epoch: 106/230 Iteration: 16100 Train loss: 0.287\n",
      "Epoch: 106/230 Iteration: 16150 Train loss: 0.307\n",
      "Epoch: 106/230 Iteration: 16200 Train loss: 0.234\n",
      "Epoch: 107/230 Iteration: 16250 Train loss: 0.252\n",
      "Epoch: 107/230 Iteration: 16300 Train loss: 0.241\n",
      "Epoch: 107/230 Iteration: 16350 Train loss: 0.254\n",
      "Epoch: 108/230 Iteration: 16400 Train loss: 0.313\n",
      "Epoch: 108/230 Iteration: 16450 Train loss: 0.258\n",
      "Epoch: 108/230 Iteration: 16500 Train loss: 0.248\n",
      "Epoch: 109/230 Iteration: 16550 Train loss: 0.259\n",
      "Epoch: 109/230 Iteration: 16600 Train loss: 0.201\n",
      "Epoch: 109/230 Iteration: 16650 Train loss: 0.263\n",
      "Epoch: 110/230 Iteration: 16700 Train loss: 0.238\n",
      "Epoch: 110/230 Iteration: 16750 Train loss: 0.232\n",
      "Epoch: 110/230 Iteration: 16800 Train loss: 0.189\n",
      "Epoch: 111/230 Iteration: 16850 Train loss: 0.232\n",
      "Epoch: 111/230 Iteration: 16900 Train loss: 0.204\n",
      "Epoch: 111/230 Iteration: 16950 Train loss: 0.208\n",
      "Epoch: 112/230 Iteration: 17000 Train loss: 0.229\n",
      "Epoch: 112/230 Iteration: 17050 Train loss: 0.263\n",
      "Epoch: 112/230 Iteration: 17100 Train loss: 0.273\n",
      "Epoch: 113/230 Iteration: 17150 Train loss: 0.207\n",
      "Epoch: 113/230 Iteration: 17200 Train loss: 0.272\n",
      "Epoch: 113/230 Iteration: 17250 Train loss: 0.225\n",
      "Epoch: 114/230 Iteration: 17300 Train loss: 0.187\n",
      "Epoch: 114/230 Iteration: 17350 Train loss: 0.231\n",
      "Epoch: 114/230 Iteration: 17400 Train loss: 0.206\n",
      "Epoch: 115/230 Iteration: 17450 Train loss: 0.219\n",
      "Epoch: 115/230 Iteration: 17500 Train loss: 0.242\n",
      "Epoch: 115/230 Iteration: 17550 Train loss: 0.258\n",
      "Epoch: 116/230 Iteration: 17600 Train loss: 0.231\n",
      "Epoch: 116/230 Iteration: 17650 Train loss: 0.274\n",
      "Epoch: 116/230 Iteration: 17700 Train loss: 0.241\n",
      "Epoch: 117/230 Iteration: 17750 Train loss: 0.251\n",
      "Epoch: 117/230 Iteration: 17800 Train loss: 0.241\n",
      "Epoch: 117/230 Iteration: 17850 Train loss: 0.239\n",
      "Epoch: 117/230 Iteration: 17900 Train loss: 0.168\n",
      "Epoch: 118/230 Iteration: 17950 Train loss: 0.234\n",
      "Epoch: 118/230 Iteration: 18000 Train loss: 0.278\n",
      "Epoch: 118/230 Iteration: 18050 Train loss: 0.238\n",
      "Epoch: 119/230 Iteration: 18100 Train loss: 0.233\n",
      "Epoch: 119/230 Iteration: 18150 Train loss: 0.295\n",
      "Epoch: 119/230 Iteration: 18200 Train loss: 0.194\n",
      "Epoch: 120/230 Iteration: 18250 Train loss: 0.263\n",
      "Epoch: 120/230 Iteration: 18300 Train loss: 0.205\n",
      "Epoch: 120/230 Iteration: 18350 Train loss: 0.229\n",
      "Epoch: 121/230 Iteration: 18400 Train loss: 0.199\n",
      "Epoch: 121/230 Iteration: 18450 Train loss: 0.246\n",
      "Epoch: 121/230 Iteration: 18500 Train loss: 0.179\n",
      "Epoch: 122/230 Iteration: 18550 Train loss: 0.237\n",
      "Epoch: 122/230 Iteration: 18600 Train loss: 0.168\n",
      "Epoch: 122/230 Iteration: 18650 Train loss: 0.268\n",
      "Epoch: 123/230 Iteration: 18700 Train loss: 0.224\n",
      "Epoch: 123/230 Iteration: 18750 Train loss: 0.214\n",
      "Epoch: 123/230 Iteration: 18800 Train loss: 0.208\n",
      "Epoch: 124/230 Iteration: 18850 Train loss: 0.233\n",
      "Epoch: 124/230 Iteration: 18900 Train loss: 0.264\n",
      "Epoch: 124/230 Iteration: 18950 Train loss: 0.232\n",
      "Epoch: 125/230 Iteration: 19000 Train loss: 0.255\n",
      "Epoch: 125/230 Iteration: 19050 Train loss: 0.229\n",
      "Epoch: 125/230 Iteration: 19100 Train loss: 0.254\n",
      "Epoch: 126/230 Iteration: 19150 Train loss: 0.253\n",
      "Epoch: 126/230 Iteration: 19200 Train loss: 0.268\n",
      "Epoch: 126/230 Iteration: 19250 Train loss: 0.214\n",
      "Epoch: 127/230 Iteration: 19300 Train loss: 0.199\n",
      "Epoch: 127/230 Iteration: 19350 Train loss: 0.229\n",
      "Epoch: 127/230 Iteration: 19400 Train loss: 0.262\n",
      "Epoch: 128/230 Iteration: 19450 Train loss: 0.221\n",
      "Epoch: 128/230 Iteration: 19500 Train loss: 0.249\n",
      "Epoch: 128/230 Iteration: 19550 Train loss: 0.268\n",
      "Epoch: 129/230 Iteration: 19600 Train loss: 0.205\n",
      "Epoch: 129/230 Iteration: 19650 Train loss: 0.211\n",
      "Epoch: 129/230 Iteration: 19700 Train loss: 0.180\n",
      "Epoch: 130/230 Iteration: 19750 Train loss: 0.270\n",
      "Epoch: 130/230 Iteration: 19800 Train loss: 0.336\n",
      "Epoch: 130/230 Iteration: 19850 Train loss: 0.251\n",
      "Epoch: 131/230 Iteration: 19900 Train loss: 0.246\n",
      "Epoch: 131/230 Iteration: 19950 Train loss: 0.201\n",
      "Epoch: 131/230 Iteration: 20000 Train loss: 0.202\n",
      "Epoch: 132/230 Iteration: 20050 Train loss: 0.241\n",
      "Epoch: 132/230 Iteration: 20100 Train loss: 0.261\n",
      "Epoch: 132/230 Iteration: 20150 Train loss: 0.221\n",
      "Epoch: 133/230 Iteration: 20200 Train loss: 0.193\n",
      "Epoch: 133/230 Iteration: 20250 Train loss: 0.268\n",
      "Epoch: 133/230 Iteration: 20300 Train loss: 0.243\n",
      "Epoch: 134/230 Iteration: 20350 Train loss: 0.277\n",
      "Epoch: 134/230 Iteration: 20400 Train loss: 0.210\n",
      "Epoch: 134/230 Iteration: 20450 Train loss: 0.271\n",
      "Epoch: 134/230 Iteration: 20500 Train loss: 0.201\n",
      "Epoch: 135/230 Iteration: 20550 Train loss: 0.228\n",
      "Epoch: 135/230 Iteration: 20600 Train loss: 0.247\n",
      "Epoch: 135/230 Iteration: 20650 Train loss: 0.211\n",
      "Epoch: 136/230 Iteration: 20700 Train loss: 0.248\n",
      "Epoch: 136/230 Iteration: 20750 Train loss: 0.247\n",
      "Epoch: 136/230 Iteration: 20800 Train loss: 0.239\n",
      "Epoch: 137/230 Iteration: 20850 Train loss: 0.232\n",
      "Epoch: 137/230 Iteration: 20900 Train loss: 0.256\n",
      "Epoch: 137/230 Iteration: 20950 Train loss: 0.211\n",
      "Epoch: 138/230 Iteration: 21000 Train loss: 0.228\n",
      "Epoch: 138/230 Iteration: 21050 Train loss: 0.243\n",
      "Epoch: 138/230 Iteration: 21100 Train loss: 0.253\n",
      "Epoch: 139/230 Iteration: 21150 Train loss: 0.252\n",
      "Epoch: 139/230 Iteration: 21200 Train loss: 0.275\n",
      "Epoch: 139/230 Iteration: 21250 Train loss: 0.236\n",
      "Epoch: 140/230 Iteration: 21300 Train loss: 0.275\n",
      "Epoch: 140/230 Iteration: 21350 Train loss: 0.229\n",
      "Epoch: 140/230 Iteration: 21400 Train loss: 0.216\n",
      "Epoch: 141/230 Iteration: 21450 Train loss: 0.279\n",
      "Epoch: 141/230 Iteration: 21500 Train loss: 0.225\n",
      "Epoch: 141/230 Iteration: 21550 Train loss: 0.239\n",
      "Epoch: 142/230 Iteration: 21600 Train loss: 0.266\n",
      "Epoch: 142/230 Iteration: 21650 Train loss: 0.257\n",
      "Epoch: 142/230 Iteration: 21700 Train loss: 0.225\n",
      "Epoch: 143/230 Iteration: 21750 Train loss: 0.226\n",
      "Epoch: 143/230 Iteration: 21800 Train loss: 0.269\n",
      "Epoch: 143/230 Iteration: 21850 Train loss: 0.235\n",
      "Epoch: 144/230 Iteration: 21900 Train loss: 0.241\n",
      "Epoch: 144/230 Iteration: 21950 Train loss: 0.211\n",
      "Epoch: 144/230 Iteration: 22000 Train loss: 0.215\n",
      "Epoch: 145/230 Iteration: 22050 Train loss: 0.188\n",
      "Epoch: 145/230 Iteration: 22100 Train loss: 0.224\n",
      "Epoch: 145/230 Iteration: 22150 Train loss: 0.265\n",
      "Epoch: 146/230 Iteration: 22200 Train loss: 0.210\n",
      "Epoch: 146/230 Iteration: 22250 Train loss: 0.204\n",
      "Epoch: 146/230 Iteration: 22300 Train loss: 0.236\n",
      "Epoch: 147/230 Iteration: 22350 Train loss: 0.212\n",
      "Epoch: 147/230 Iteration: 22400 Train loss: 0.241\n",
      "Epoch: 147/230 Iteration: 22450 Train loss: 0.221\n",
      "Epoch: 148/230 Iteration: 22500 Train loss: 0.204\n",
      "Epoch: 148/230 Iteration: 22550 Train loss: 0.212\n",
      "Epoch: 148/230 Iteration: 22600 Train loss: 0.175\n",
      "Epoch: 149/230 Iteration: 22650 Train loss: 0.220\n",
      "Epoch: 149/230 Iteration: 22700 Train loss: 0.213\n",
      "Epoch: 149/230 Iteration: 22750 Train loss: 0.243\n",
      "Epoch: 150/230 Iteration: 22800 Train loss: 0.274\n",
      "Epoch: 150/230 Iteration: 22850 Train loss: 0.278\n",
      "Epoch: 150/230 Iteration: 22900 Train loss: 0.242\n",
      "Epoch: 151/230 Iteration: 22950 Train loss: 0.194\n",
      "Epoch: 151/230 Iteration: 23000 Train loss: 0.289\n",
      "Epoch: 151/230 Iteration: 23050 Train loss: 0.204\n",
      "Epoch: 151/230 Iteration: 23100 Train loss: 0.284\n",
      "Epoch: 152/230 Iteration: 23150 Train loss: 0.209\n",
      "Epoch: 152/230 Iteration: 23200 Train loss: 0.225\n",
      "Epoch: 152/230 Iteration: 23250 Train loss: 0.209\n",
      "Epoch: 153/230 Iteration: 23300 Train loss: 0.249\n",
      "Epoch: 153/230 Iteration: 23350 Train loss: 0.270\n",
      "Epoch: 153/230 Iteration: 23400 Train loss: 0.219\n",
      "Epoch: 154/230 Iteration: 23450 Train loss: 0.226\n",
      "Epoch: 154/230 Iteration: 23500 Train loss: 0.267\n",
      "Epoch: 154/230 Iteration: 23550 Train loss: 0.206\n",
      "Epoch: 155/230 Iteration: 23600 Train loss: 0.240\n",
      "Epoch: 155/230 Iteration: 23650 Train loss: 0.233\n",
      "Epoch: 155/230 Iteration: 23700 Train loss: 0.209\n",
      "Epoch: 156/230 Iteration: 23750 Train loss: 0.250\n",
      "Epoch: 156/230 Iteration: 23800 Train loss: 0.247\n",
      "Epoch: 156/230 Iteration: 23850 Train loss: 0.235\n",
      "Epoch: 157/230 Iteration: 23900 Train loss: 0.218\n",
      "Epoch: 157/230 Iteration: 23950 Train loss: 0.266\n",
      "Epoch: 157/230 Iteration: 24000 Train loss: 0.310\n",
      "Epoch: 158/230 Iteration: 24050 Train loss: 0.265\n",
      "Epoch: 158/230 Iteration: 24100 Train loss: 0.254\n",
      "Epoch: 158/230 Iteration: 24150 Train loss: 0.220\n",
      "Epoch: 159/230 Iteration: 24200 Train loss: 0.236\n",
      "Epoch: 159/230 Iteration: 24250 Train loss: 0.256\n",
      "Epoch: 159/230 Iteration: 24300 Train loss: 0.229\n",
      "Epoch: 160/230 Iteration: 24350 Train loss: 0.261\n",
      "Epoch: 160/230 Iteration: 24400 Train loss: 0.251\n",
      "Epoch: 160/230 Iteration: 24450 Train loss: 0.215\n",
      "Epoch: 161/230 Iteration: 24500 Train loss: 0.279\n",
      "Epoch: 161/230 Iteration: 24550 Train loss: 0.218\n",
      "Epoch: 161/230 Iteration: 24600 Train loss: 0.194\n",
      "Epoch: 162/230 Iteration: 24650 Train loss: 0.226\n",
      "Epoch: 162/230 Iteration: 24700 Train loss: 0.351\n",
      "Epoch: 162/230 Iteration: 24750 Train loss: 0.257\n",
      "Epoch: 163/230 Iteration: 24800 Train loss: 0.314\n",
      "Epoch: 163/230 Iteration: 24850 Train loss: 0.202\n",
      "Epoch: 163/230 Iteration: 24900 Train loss: 0.228\n",
      "Epoch: 164/230 Iteration: 24950 Train loss: 0.204\n",
      "Epoch: 164/230 Iteration: 25000 Train loss: 0.235\n",
      "Epoch: 164/230 Iteration: 25050 Train loss: 0.213\n",
      "Epoch: 165/230 Iteration: 25100 Train loss: 0.256\n",
      "Epoch: 165/230 Iteration: 25150 Train loss: 0.216\n",
      "Epoch: 165/230 Iteration: 25200 Train loss: 0.221\n",
      "Epoch: 166/230 Iteration: 25250 Train loss: 0.253\n",
      "Epoch: 166/230 Iteration: 25300 Train loss: 0.191\n",
      "Epoch: 166/230 Iteration: 25350 Train loss: 0.248\n",
      "Epoch: 167/230 Iteration: 25400 Train loss: 0.310\n",
      "Epoch: 167/230 Iteration: 25450 Train loss: 0.266\n",
      "Epoch: 167/230 Iteration: 25500 Train loss: 0.246\n",
      "Epoch: 167/230 Iteration: 25550 Train loss: 0.167\n",
      "Epoch: 168/230 Iteration: 25600 Train loss: 0.230\n",
      "Epoch: 168/230 Iteration: 25650 Train loss: 0.252\n",
      "Epoch: 168/230 Iteration: 25700 Train loss: 0.222\n",
      "Epoch: 169/230 Iteration: 25750 Train loss: 0.191\n",
      "Epoch: 169/230 Iteration: 25800 Train loss: 0.275\n",
      "Epoch: 169/230 Iteration: 25850 Train loss: 0.225\n",
      "Epoch: 170/230 Iteration: 25900 Train loss: 0.236\n",
      "Epoch: 170/230 Iteration: 25950 Train loss: 0.193\n",
      "Epoch: 170/230 Iteration: 26000 Train loss: 0.222\n",
      "Epoch: 171/230 Iteration: 26050 Train loss: 0.207\n",
      "Epoch: 171/230 Iteration: 26100 Train loss: 0.293\n",
      "Epoch: 171/230 Iteration: 26150 Train loss: 0.166\n",
      "Epoch: 172/230 Iteration: 26200 Train loss: 0.211\n",
      "Epoch: 172/230 Iteration: 26250 Train loss: 0.166\n",
      "Epoch: 172/230 Iteration: 26300 Train loss: 0.276\n",
      "Epoch: 173/230 Iteration: 26350 Train loss: 0.211\n",
      "Epoch: 173/230 Iteration: 26400 Train loss: 0.199\n",
      "Epoch: 173/230 Iteration: 26450 Train loss: 0.217\n",
      "Epoch: 174/230 Iteration: 26500 Train loss: 0.232\n",
      "Epoch: 174/230 Iteration: 26550 Train loss: 0.261\n",
      "Epoch: 174/230 Iteration: 26600 Train loss: 0.224\n",
      "Epoch: 175/230 Iteration: 26650 Train loss: 0.239\n",
      "Epoch: 175/230 Iteration: 26700 Train loss: 0.207\n",
      "Epoch: 175/230 Iteration: 26750 Train loss: 0.224\n",
      "Epoch: 176/230 Iteration: 26800 Train loss: 0.258\n",
      "Epoch: 176/230 Iteration: 26850 Train loss: 0.258\n",
      "Epoch: 176/230 Iteration: 26900 Train loss: 0.229\n",
      "Epoch: 177/230 Iteration: 26950 Train loss: 0.192\n",
      "Epoch: 177/230 Iteration: 27000 Train loss: 0.242\n",
      "Epoch: 177/230 Iteration: 27050 Train loss: 0.258\n",
      "Epoch: 178/230 Iteration: 27100 Train loss: 0.229\n",
      "Epoch: 178/230 Iteration: 27150 Train loss: 0.242\n",
      "Epoch: 178/230 Iteration: 27200 Train loss: 0.232\n",
      "Epoch: 179/230 Iteration: 27250 Train loss: 0.213\n",
      "Epoch: 179/230 Iteration: 27300 Train loss: 0.195\n",
      "Epoch: 179/230 Iteration: 27350 Train loss: 0.219\n",
      "Epoch: 180/230 Iteration: 27400 Train loss: 0.260\n",
      "Epoch: 180/230 Iteration: 27450 Train loss: 0.380\n",
      "Epoch: 180/230 Iteration: 27500 Train loss: 0.186\n",
      "Epoch: 181/230 Iteration: 27550 Train loss: 0.217\n",
      "Epoch: 181/230 Iteration: 27600 Train loss: 0.295\n",
      "Epoch: 181/230 Iteration: 27650 Train loss: 0.200\n",
      "Epoch: 182/230 Iteration: 27700 Train loss: 0.185\n",
      "Epoch: 182/230 Iteration: 27750 Train loss: 0.201\n",
      "Epoch: 182/230 Iteration: 27800 Train loss: 0.176\n",
      "Epoch: 183/230 Iteration: 27850 Train loss: 0.201\n",
      "Epoch: 183/230 Iteration: 27900 Train loss: 0.201\n",
      "Epoch: 183/230 Iteration: 27950 Train loss: 0.218\n",
      "Epoch: 184/230 Iteration: 28000 Train loss: 0.252\n",
      "Epoch: 184/230 Iteration: 28050 Train loss: 0.155\n",
      "Epoch: 184/230 Iteration: 28100 Train loss: 0.260\n",
      "Epoch: 184/230 Iteration: 28150 Train loss: 0.202\n",
      "Epoch: 185/230 Iteration: 28200 Train loss: 0.211\n",
      "Epoch: 185/230 Iteration: 28250 Train loss: 0.212\n",
      "Epoch: 185/230 Iteration: 28300 Train loss: 0.193\n",
      "Epoch: 186/230 Iteration: 28350 Train loss: 0.271\n",
      "Epoch: 186/230 Iteration: 28400 Train loss: 0.188\n",
      "Epoch: 186/230 Iteration: 28450 Train loss: 0.214\n",
      "Epoch: 187/230 Iteration: 28500 Train loss: 0.276\n",
      "Epoch: 187/230 Iteration: 28550 Train loss: 0.233\n",
      "Epoch: 187/230 Iteration: 28600 Train loss: 0.216\n",
      "Epoch: 188/230 Iteration: 28650 Train loss: 0.207\n",
      "Epoch: 188/230 Iteration: 28700 Train loss: 0.221\n",
      "Epoch: 188/230 Iteration: 28750 Train loss: 0.213\n",
      "Epoch: 189/230 Iteration: 28800 Train loss: 0.224\n",
      "Epoch: 189/230 Iteration: 28850 Train loss: 0.317\n",
      "Epoch: 189/230 Iteration: 28900 Train loss: 0.213\n",
      "Epoch: 190/230 Iteration: 28950 Train loss: 0.234\n",
      "Epoch: 190/230 Iteration: 29000 Train loss: 0.202\n",
      "Epoch: 190/230 Iteration: 29050 Train loss: 0.214\n",
      "Epoch: 191/230 Iteration: 29100 Train loss: 0.254\n",
      "Epoch: 191/230 Iteration: 29150 Train loss: 0.262\n",
      "Epoch: 191/230 Iteration: 29200 Train loss: 0.220\n",
      "Epoch: 192/230 Iteration: 29250 Train loss: 0.261\n",
      "Epoch: 192/230 Iteration: 29300 Train loss: 0.255\n",
      "Epoch: 192/230 Iteration: 29350 Train loss: 0.219\n",
      "Epoch: 193/230 Iteration: 29400 Train loss: 0.209\n",
      "Epoch: 193/230 Iteration: 29450 Train loss: 0.184\n",
      "Epoch: 193/230 Iteration: 29500 Train loss: 0.287\n",
      "Epoch: 194/230 Iteration: 29550 Train loss: 0.230\n",
      "Epoch: 194/230 Iteration: 29600 Train loss: 0.230\n",
      "Epoch: 194/230 Iteration: 29650 Train loss: 0.211\n",
      "Epoch: 195/230 Iteration: 29700 Train loss: 0.195\n",
      "Epoch: 195/230 Iteration: 29750 Train loss: 0.225\n",
      "Epoch: 195/230 Iteration: 29800 Train loss: 0.323\n",
      "Epoch: 196/230 Iteration: 29850 Train loss: 0.230\n",
      "Epoch: 196/230 Iteration: 29900 Train loss: 0.183\n",
      "Epoch: 196/230 Iteration: 29950 Train loss: 0.235\n",
      "Epoch: 197/230 Iteration: 30000 Train loss: 0.209\n",
      "Epoch: 197/230 Iteration: 30050 Train loss: 0.235\n",
      "Epoch: 197/230 Iteration: 30100 Train loss: 0.201\n",
      "Epoch: 198/230 Iteration: 30150 Train loss: 0.235\n",
      "Epoch: 198/230 Iteration: 30200 Train loss: 0.234\n",
      "Epoch: 198/230 Iteration: 30250 Train loss: 0.191\n",
      "Epoch: 199/230 Iteration: 30300 Train loss: 0.163\n",
      "Epoch: 199/230 Iteration: 30350 Train loss: 0.205\n",
      "Epoch: 199/230 Iteration: 30400 Train loss: 0.216\n",
      "Epoch: 200/230 Iteration: 30450 Train loss: 0.250\n",
      "Epoch: 200/230 Iteration: 30500 Train loss: 0.277\n",
      "Epoch: 200/230 Iteration: 30550 Train loss: 0.195\n",
      "Epoch: 201/230 Iteration: 30600 Train loss: 0.179\n",
      "Epoch: 201/230 Iteration: 30650 Train loss: 0.216\n",
      "Epoch: 201/230 Iteration: 30700 Train loss: 0.258\n",
      "Epoch: 201/230 Iteration: 30750 Train loss: 0.256\n",
      "Epoch: 202/230 Iteration: 30800 Train loss: 0.190\n",
      "Epoch: 202/230 Iteration: 30850 Train loss: 0.228\n",
      "Epoch: 202/230 Iteration: 30900 Train loss: 0.183\n",
      "Epoch: 203/230 Iteration: 30950 Train loss: 0.226\n",
      "Epoch: 203/230 Iteration: 31000 Train loss: 0.268\n",
      "Epoch: 203/230 Iteration: 31050 Train loss: 0.199\n",
      "Epoch: 204/230 Iteration: 31100 Train loss: 0.161\n",
      "Epoch: 204/230 Iteration: 31150 Train loss: 0.264\n",
      "Epoch: 204/230 Iteration: 31200 Train loss: 0.254\n",
      "Epoch: 205/230 Iteration: 31250 Train loss: 0.259\n",
      "Epoch: 205/230 Iteration: 31300 Train loss: 0.239\n",
      "Epoch: 205/230 Iteration: 31350 Train loss: 0.219\n",
      "Epoch: 206/230 Iteration: 31400 Train loss: 0.205\n",
      "Epoch: 206/230 Iteration: 31450 Train loss: 0.247\n",
      "Epoch: 206/230 Iteration: 31500 Train loss: 0.215\n",
      "Epoch: 207/230 Iteration: 31550 Train loss: 0.229\n",
      "Epoch: 207/230 Iteration: 31600 Train loss: 0.245\n",
      "Epoch: 207/230 Iteration: 31650 Train loss: 0.231\n",
      "Epoch: 208/230 Iteration: 31700 Train loss: 0.284\n",
      "Epoch: 208/230 Iteration: 31750 Train loss: 0.253\n",
      "Epoch: 208/230 Iteration: 31800 Train loss: 0.222\n",
      "Epoch: 209/230 Iteration: 31850 Train loss: 0.231\n",
      "Epoch: 209/230 Iteration: 31900 Train loss: 0.170\n",
      "Epoch: 209/230 Iteration: 31950 Train loss: 0.233\n",
      "Epoch: 210/230 Iteration: 32000 Train loss: 0.231\n",
      "Epoch: 210/230 Iteration: 32050 Train loss: 0.216\n",
      "Epoch: 210/230 Iteration: 32100 Train loss: 0.233\n",
      "Epoch: 211/230 Iteration: 32150 Train loss: 0.212\n",
      "Epoch: 211/230 Iteration: 32200 Train loss: 0.236\n",
      "Epoch: 211/230 Iteration: 32250 Train loss: 0.201\n",
      "Epoch: 212/230 Iteration: 32300 Train loss: 0.206\n",
      "Epoch: 212/230 Iteration: 32350 Train loss: 0.216\n",
      "Epoch: 212/230 Iteration: 32400 Train loss: 0.226\n",
      "Epoch: 213/230 Iteration: 32450 Train loss: 0.209\n",
      "Epoch: 213/230 Iteration: 32500 Train loss: 0.220\n",
      "Epoch: 213/230 Iteration: 32550 Train loss: 0.214\n",
      "Epoch: 214/230 Iteration: 32600 Train loss: 0.168\n",
      "Epoch: 214/230 Iteration: 32650 Train loss: 0.202\n",
      "Epoch: 214/230 Iteration: 32700 Train loss: 0.211\n",
      "Epoch: 215/230 Iteration: 32750 Train loss: 0.223\n",
      "Epoch: 215/230 Iteration: 32800 Train loss: 0.208\n",
      "Epoch: 215/230 Iteration: 32850 Train loss: 0.251\n",
      "Epoch: 216/230 Iteration: 32900 Train loss: 0.211\n",
      "Epoch: 216/230 Iteration: 32950 Train loss: 0.198\n",
      "Epoch: 216/230 Iteration: 33000 Train loss: 0.243\n",
      "Epoch: 217/230 Iteration: 33050 Train loss: 0.217\n",
      "Epoch: 217/230 Iteration: 33100 Train loss: 0.287\n",
      "Epoch: 217/230 Iteration: 33150 Train loss: 0.263\n",
      "Epoch: 217/230 Iteration: 33200 Train loss: 0.232\n",
      "Epoch: 218/230 Iteration: 33250 Train loss: 0.189\n",
      "Epoch: 218/230 Iteration: 33300 Train loss: 0.281\n",
      "Epoch: 218/230 Iteration: 33350 Train loss: 0.244\n",
      "Epoch: 219/230 Iteration: 33400 Train loss: 0.204\n",
      "Epoch: 219/230 Iteration: 33450 Train loss: 0.304\n",
      "Epoch: 219/230 Iteration: 33500 Train loss: 0.196\n",
      "Epoch: 220/230 Iteration: 33550 Train loss: 0.268\n",
      "Epoch: 220/230 Iteration: 33600 Train loss: 0.184\n",
      "Epoch: 220/230 Iteration: 33650 Train loss: 0.222\n",
      "Epoch: 221/230 Iteration: 33700 Train loss: 0.202\n",
      "Epoch: 221/230 Iteration: 33750 Train loss: 0.268\n",
      "Epoch: 221/230 Iteration: 33800 Train loss: 0.209\n",
      "Epoch: 222/230 Iteration: 33850 Train loss: 0.247\n",
      "Epoch: 222/230 Iteration: 33900 Train loss: 0.210\n",
      "Epoch: 222/230 Iteration: 33950 Train loss: 0.250\n",
      "Epoch: 223/230 Iteration: 34000 Train loss: 0.221\n",
      "Epoch: 223/230 Iteration: 34050 Train loss: 0.220\n",
      "Epoch: 223/230 Iteration: 34100 Train loss: 0.219\n",
      "Epoch: 224/230 Iteration: 34150 Train loss: 0.220\n",
      "Epoch: 224/230 Iteration: 34200 Train loss: 0.282\n",
      "Epoch: 224/230 Iteration: 34250 Train loss: 0.219\n",
      "Epoch: 225/230 Iteration: 34300 Train loss: 0.269\n",
      "Epoch: 225/230 Iteration: 34350 Train loss: 0.194\n",
      "Epoch: 225/230 Iteration: 34400 Train loss: 0.202\n",
      "Epoch: 226/230 Iteration: 34450 Train loss: 0.251\n",
      "Epoch: 226/230 Iteration: 34500 Train loss: 0.242\n",
      "Epoch: 226/230 Iteration: 34550 Train loss: 0.203\n",
      "Epoch: 227/230 Iteration: 34600 Train loss: 0.183\n",
      "Epoch: 227/230 Iteration: 34650 Train loss: 0.219\n",
      "Epoch: 227/230 Iteration: 34700 Train loss: 0.214\n",
      "Epoch: 228/230 Iteration: 34750 Train loss: 0.223\n",
      "Epoch: 228/230 Iteration: 34800 Train loss: 0.237\n",
      "Epoch: 228/230 Iteration: 34850 Train loss: 0.239\n",
      "Epoch: 229/230 Iteration: 34900 Train loss: 0.187\n",
      "Epoch: 229/230 Iteration: 34950 Train loss: 0.232\n",
      "Epoch: 229/230 Iteration: 35000 Train loss: 0.185\n",
      "Epoch: 230/230 Iteration: 35050 Train loss: 0.258\n",
      "Epoch: 230/230 Iteration: 35100 Train loss: 0.315\n",
      "Epoch: 230/230 Iteration: 35150 Train loss: 0.226\n",
      "duration: 598.4 sec\n"
     ]
    }
   ],
   "source": [
    "epochs = 230\n",
    "\n",
    "model = CharLSTM(vocab_size, batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, lstm_layers=lstm_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=10000) \n",
    "\n",
    "\n",
    "loss_vec = []\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    \n",
    "    start_time=time.time()\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        state = sess.run(model.initial_state) \n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(features_x, features_y, batch_size), 1):\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.initial_state: state}\n",
    "            batch_loss, state, _, initial_state_val = sess.run([model.loss,                                    \n",
    "                                        model.final_state,model.optimizer,model.initial_state],\n",
    "                                         feed_dict=feed)\n",
    "            \n",
    "            loss_vec.append(batch_loss)\n",
    "           \n",
    "            \n",
    "            if iteration%50==0:\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(batch_loss))\n",
    "            \n",
    "            # we save checkpoints that we will use to perform the sampling\n",
    "            if (iteration % 2000 == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}.ckpt\".format(iteration))\n",
    "                \n",
    "            iteration +=1\n",
    "            \n",
    "    \n",
    "    duration=time.time()-start_time\n",
    "    print(\"duration: {:.1f} sec\".format(duration))\n",
    "    saver.save(sess, \"checkpoints/i{}.ckpt\".format(iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plot training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(np.arange(1,iteration+1), loss_vec)\n",
    "plt.ylabel('losses')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sampling\n",
    "Now that our model is trained we would like to generate new text (characters). The process is explained in the figure below:\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "Assuming the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step and have the network then sample one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint,lstm_size, vocab_size):\n",
    "    \n",
    "    samples = []\n",
    "    model = CharLSTM(num_classes=vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "        \n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state) \n",
    "\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0,0] = -1\n",
    "        feed = {model.inputs: x,\n",
    "                model.initial_state: new_state}\n",
    "        preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "        \n",
    "                \n",
    "        c = pick_top_n(preds, vocab_size)\n",
    "        samples.append(ix_to_char[c])\n",
    "        counter=1\n",
    "\n",
    "        \n",
    "        #for i in range(n_samples):\n",
    "        while (c != char_to_ix['\\n'] and counter != num_steps):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, vocab_size)\n",
    "            samples.append(ix_to_char[c])\n",
    "        \n",
    "            counter +=1\n",
    "            \n",
    "        if (counter == num_steps):\n",
    "            samples.append('\\n')\n",
    "           \n",
    "            \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### After a few thousand iterations the model learns to generate reasonable-looking names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ur\n",
      "\n",
      "ausscus\n",
      "\n",
      "rswuaurvs\n",
      "\n",
      "u\n",
      "\n",
      "ruass\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 2000\n",
      "\n",
      "inototinodon\n",
      "\n",
      "otian\n",
      "\n",
      "oraptor\n",
      "\n",
      "angatosaurus\n",
      "\n",
      "elocaelius\n",
      "\n",
      "eloteris\n",
      "\n",
      "anchuanosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 4000\n",
      "\n",
      "acherodon\n",
      "\n",
      "ampelosaurus\n",
      "\n",
      "oceratops\n",
      "\n",
      "ischirosteornithomimus\n",
      "\n",
      "abrrasaurus\n",
      "\n",
      "eonia\n",
      "\n",
      "racodrimmodracodon\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 6000\n",
      "\n",
      "abisaurus\n",
      "\n",
      "antrodesaurus\n",
      "\n",
      "ananosaurus\n",
      "\n",
      "rugops\n",
      "\n",
      "achelousaurus\n",
      "\n",
      "elmadon\n",
      "\n",
      "abdallahsaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 8000\n",
      "\n",
      "bellesodon\n",
      "\n",
      "anatosaurus\n",
      "\n",
      "antarkania\n",
      "\n",
      "osaurus\n",
      "\n",
      "anartapasaurus\n",
      "\n",
      "rachilestes\n",
      "\n",
      "bachisaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 10000\n",
      "\n",
      "rachilisaurus\n",
      "\n",
      "oxalosaurus\n",
      "\n",
      "abrosaurus\n",
      "\n",
      "abdanlong\n",
      "\n",
      "eningosaurus\n",
      "\n",
      "ravia\n",
      "\n",
      "andhodasaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 12000\n",
      "\n",
      "arambatitan\n",
      "\n",
      "bradnyraptor\n",
      "\n",
      "eoceratops\n",
      "\n",
      "rubilengoraptor\n",
      "\n",
      "acheroraptor\n",
      "\n",
      "angolatitan\n",
      "\n",
      "osaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 14000\n",
      "\n",
      "rachilingsaurus\n",
      "\n",
      "erlanceratops\n",
      "\n",
      "opistesiatoraptor\n",
      "\n",
      "opalasaura\n",
      "\n",
      "achelousaurus\n",
      "\n",
      "acheibeeleoceratops\n",
      "\n",
      "orebeatrus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 16000\n",
      "\n",
      "elopteryx\n",
      "\n",
      "achillosaurus\n",
      "\n",
      "actiosaurus\n",
      "\n",
      "ellusaurus\n",
      "\n",
      "raptorevochisaurus\n",
      "\n",
      "ornithomimus\n",
      "\n",
      "erenosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 18000\n",
      "\n",
      "abelisaurus\n",
      "\n",
      "archaeobehongosaurus\n",
      "\n",
      "anchodon\n",
      "\n",
      "ilong\n",
      "\n",
      "abydosaurus\n",
      "\n",
      "erentosaurus\n",
      "\n",
      "erentosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 20000\n",
      "\n",
      "rachinosaurus\n",
      "\n",
      "raptor\n",
      "\n",
      "achenosaurus\n",
      "\n",
      "archaeopteryx\n",
      "\n",
      "abrictosaurus\n",
      "\n",
      "rachitosaurus\n",
      "\n",
      "abrosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 22000\n",
      "\n",
      "abydosaurusotheirinasaurusinos\n",
      "\n",
      "achelousaurus\n",
      "\n",
      "ornatotholurion\n",
      "\n",
      "endapasaurus\n",
      "\n",
      "eoplosaurus\n",
      "\n",
      "eobriceratops\n",
      "\n",
      "achenocosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 24000\n",
      "\n",
      "achelousaurus\n",
      "\n",
      "athyrosaurus\n",
      "\n",
      "endornierimus\n",
      "\n",
      "achilong\n",
      "\n",
      "achenolosaurus\n",
      "\n",
      "rabysaurus\n",
      "\n",
      "oviraptor\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 26000\n",
      "\n",
      "abrosaurus\n",
      "\n",
      "anthodon\n",
      "\n",
      "osauru\n",
      "\n",
      "acanthopholis\n",
      "\n",
      "achelousaurus\n",
      "\n",
      "archaeocephale\n",
      "\n",
      "andesaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 28000\n",
      "\n",
      "rachetriceratops\n",
      "\n",
      "acanthosaurus\n",
      "\n",
      "hichocoelurus\n",
      "\n",
      "antrosaurus\n",
      "\n",
      "amargasaurus\n",
      "\n",
      "rachetor\n",
      "\n",
      "anosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 30000\n",
      "\n",
      "haplotholus\n",
      "\n",
      "radomaurus\n",
      "\n",
      "abrictosaurus\n",
      "\n",
      "antyodon\n",
      "\n",
      "athygnathus\n",
      "\n",
      "endiceratops\n",
      "\n",
      "abisaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 32000\n",
      "\n",
      "athyngosaurus\n",
      "\n",
      "rachilonasaurus\n",
      "\n",
      "anasaura\n",
      "\n",
      "aceratops\n",
      "\n",
      "opistahisaurus\n",
      "\n",
      "athyngosaurus\n",
      "\n",
      "abrictosaurus\n",
      "\n",
      "--------------\n",
      "\n",
      "iteration: 34000\n",
      "\n",
      "abrictosaurus\n",
      "\n",
      "becenterias\n",
      "\n",
      "rinhosaurus\n",
      "\n",
      "acanthosaurus\n",
      "\n",
      "barradicasaurus\n",
      "\n",
      "atlungia\n",
      "\n",
      "abelosaurus\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dino_names = 7 # we will print 7 dinosaur names for each checkpoints\n",
    "\n",
    "for kkk in range(0, iteration, 2000):\n",
    "    checkpoint = 'checkpoints/i'+str(kkk)+'.ckpt'\n",
    "    print('iteration: '+str(kkk)+'\\n')\n",
    "    for name in range(dino_names):\n",
    "        samp = sample(checkpoint,lstm_size, vocab_size)\n",
    "        print(samp)\n",
    "    print('--------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The dinosaur names generated at the last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archaeornis\n",
      "\n",
      "heceratiops\n",
      "\n",
      "entosaurus\n",
      "\n",
      "archaeophus\n",
      "\n",
      "epicryposaurus\n",
      "\n",
      "edmygonosaurus\n",
      "\n",
      "abdallahsaurus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "for name in range(dino_names):\n",
    "    samp = sample(checkpoint,lstm_size, vocab_size)\n",
    "    print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
